% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sure_trendfilter.R
\name{sure_trendfilter}
\alias{sure_trendfilter}
\title{Optimize the trend filtering hyperparameter by minimizing Stein's unbiased
risk estimate}
\usage{
sure_trendfilter(x, y, weights, k = 2L, nlambdas = 250L, optimization_params)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal in \code{y}.
When the noise is expected to have equal variance for all observations,
\code{weights} can be passed as a scalar. Otherwise, \code{weights} must be a vector
with the same length as \code{x} and \code{y}.}

\item{k}{Degree of the polynomials that make up the piecewise-polynomial
trend filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{Number of hyperparameter values to test during validation.
Defaults to \code{nlambdas = 250}. The hyperparameter grid is dynamically
constructed to span the full model space lying between a single polynomial
solution (i.e. a power law) and an interpolating solution, with \code{nlambdas}
controlling the granularity of the grid.}

\item{optimization_params}{(Optional) A named list of parameter values to be
passed to the trend filtering ADMM algorithm of
\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and Tibshirani (2016)}, which is implemented in
the \code{glmgen} R package. See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for a full list of the algorithm's parameters. The default
parameter choices will almost always suffice, but when adjustments do need to
be made, one can do so without any technical understanding of the ADMM
algorithm. In particular, the four parameter descriptions below should serve
as sufficient working knowledge.
\describe{
\item{obj_tol}{A stopping threshold for the ADMM algorithm. If the relative
change in the algorithm's cost functional between two consecutive steps is
less than \code{obj_tol}, the algorithm terminates. The algorithm's termination
can also result from it reaching the maximum tolerable iterations set
by the \code{max_iter} parameter (see below). The \code{obj_tol} parameter defaults to
\code{obj_tol = 1e-10}. The \code{cost_functional} vector, returned within the
\code{sure_trendfilter()} output, gives the relative change in the trend filtering
cost functional over the algorithm's final iteration, for every candidate
hyperparameter value.}
\item{max_iter}{Maximum number of ADMM iterations that we will tolerate.
Defaults to \code{max_iter = length(y)}. The actual number of iterations performed
by the algorithm, for every candidate hyperparameter value, is returned in
the \code{n_iter} vector, within the \code{sure_trendfilter()} output. If any of the
elements of \code{n_iter} are equal to \code{max_iter}, the tolerance defined by
\code{obj_tol} has not been attained and \code{max_iter} may need to be increased.}
\item{thinning}{Logical. If \code{TRUE}, then the data are preprocessed so that
a smaller, better-conditioned data set is used to fit the trend filtering
estimate. This can be very useful when a data set samples a signal so well
that very little additional information / predictive accuracy is gained
by fitting the trend filtering estimate on the full data set, compared to
some fraction of it. See the \code{\link[=cv_trendfilter]{cv_trendfilter()}} examples for a case study
of this nature, when the goal is to denoise the light curve (i.e. a
brightness time series) of a binary star system, after it has been folded
(i.e. stacked) based on the orbital period of the star system. When left
\code{thinning = NULL} (the default setting), the optimization will automatically
detect whether thinning should be applied (i.e. cases in which the numerical
fitting algorithm will struggle to converge). This preprocessing procedure is
controlled by the \code{x_tol} argument below.}
\item{x_tol}{Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then the data is thinned.
}}}
}
\value{
An object of class \code{sure_tf}. This is a list with the following
elements:
\describe{
\item{lambdas}{Vector of hyperparameter values evaluated in the grid search
(always returned in descending order).}
\item{edfs}{Vector of effective degrees of freedom for all trend filtering
estimators fit during model validation.}
\item{generalization_errors}{Vector of SURE generalization error estimates,
ordered according to the descending-ordered \code{lambdas} vector.}
\item{se_errors}{The standard errors of the SURE estimates of the trend
filtering model's generalization error for every candidate hyperparameter
value in \code{lambdas}. These are particularly useful for implementing the
"1-standard-error rule".}
\item{lambda_min}{Hyperparameter value that minimizes the SURE generalization
error curve.}
\item{lambda_1se}{Largest hyperparameter value that is within one standard
error of the minimum hyperparameter's cross validation error.}
\item{edf_min}{Effective degrees of freedom of the optimized trend
filtering estimator.}
\item{i_min}{Index of \code{lambdas} that minimizes the SURE error curve.}
\item{i_1se}{Index of \code{lambdas} that gives the largest hyperparameter
value that has a cross validation error within 1 standard error of the
minimum of the cross validation error curves.}
\item{edf_min}{Effective degrees of freedom of the optimized trend
filtering estimator.}
\item{edf_1se}{Effective degrees of freedom of the 1-stand-error rule
trend filtering estimator.}
\item{cost_functional}{The relative change in the cost functional values
between the ADMM algorithm penultimate and ultimate state, for every
hyperparameter choice.}
\item{n_iter}{The number of iterations taken by the ADMM algorithm, for
every candidate hyperparameter value in \code{lambdas}. If an element of \code{n_iter}
is exactly equal to \code{max_iter}, then the ADMM algorithm stopped before
reaching the tolerance specified by \code{obj_tol}. In these cases, you may need
to increase \code{max_iter}.}
\item{training_errors}{Mean-squared error between the observed outputs \code{y}
and the trend filtering estimate, for every hyperparameter choice.}
\item{optimisms}{SURE-estimated optimisms, i.e.
\code{optimisms = generalization_errors - training_errors}.}
\item{x}{Vector of observed inputs.}
\item{y}{Vector of observed outputs.}
\item{weights}{Vector of weights for the observed outputs.}
\item{k}{Degree of the trend filtering estimator.}
\item{admm_params}{List of parameter settings for the trend filtering ADMM
algorithm, constructed by passing the \code{optimization_params} list to
\code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}.}
\item{thinning}{Logical. If \code{TRUE}, then the data were preprocessed such
that a reduced subset was passed to the trend filtering ADMM algorithm in
order to make for a more tractable/stable problem and solution.}
\item{x_scale, y_scale, data_scaled}{For internal use.}
}
}
\description{
For every candidate hyperparameter value, compute an unbiased estimate of the
trend filtering model's predictive mean-squared error. See details below
for when you should use \code{\link[=sure_trendfilter]{sure_trendfilter()}} versus \code{\link[=cv_trendfilter]{cv_trendfilter()}}.
}
\details{
Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} vs.
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are shown in the table below.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in large consecutive chunks) is still considered to
be evenly sampled. When the inputs are evenly sampled on a transformed scale,
we recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the example below for a case when the
inputs are evenly sampled on the \code{log10(x)} scale.
}
\examples{
data(quasar_spectrum)
head(spec)

sure_tf <- sure_trendfilter(spec$log10_wavelength, spec$flux, spec$weights)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Stein's unbiased risk estimate}
\enumerate{
\item{Tibshirani and Wasserman (2015).
\href{http://www.stat.cmu.edu/~larry/=sml/stein.pdf}{Stein’s Unbiased Risk
Estimate}. \emph{36-702: Statistical Machine Learning course notes}
(Carnegie Mellon University).} \cr
\item{Efron (2014).
\href{https://www.tandfonline.com/doi/abs/10.1198/016214504000000692}{
The Estimation of Prediction Error: Covariance Penalties
and Cross-Validation}. \emph{Journal of the American Statistical
Association}. 99(467), p. 619-632.} \cr
\item{Stein (1981).
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-6/Estimation-of-the-Mean-of-a-Multivariate-Normal-Distribution/10.1214/aos/1176345632.full}{
Estimation of the Mean of a Multivariate Normal Distribution}.
\emph{The Annals of Statistics}. 9(6), p. 1135-1151.}}

\bold{Effective degrees of freedom for trend filtering}
\enumerate{
\item{Tibshirani and Taylor (2012)}.
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-2/Degrees-of-freedom-in-lasso-problems/10.1214/12-AOS1003.full}{
Degrees of freedom in lasso problems}. \emph{The Annals of Statistics},
40(2), p. 1198-1232.}
}
\seealso{
\code{\link[=cv_trendfilter]{cv_trendfilter()}}, \code{\link[=bootstrap_trendfilter]{bootstrap_trendfilter()}}
}
