% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sure_trendfilter.R
\name{sure_trendfilter}
\alias{sure_trendfilter}
\title{Optimize the trend filtering hyperparameter by minimizing Stein's unbiased
risk estimate}
\usage{
sure_trendfilter(
  x,
  y,
  weights,
  k = 2L,
  nlambdas = 250L,
  nx_eval = 1500L,
  x_eval,
  optimization_params
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal in \code{y}.
When the noise is expected to have equal variance for all observations,
\code{weights} can be passed as a scalar. Otherwise, \code{weights} must have the same
length as \code{x} and \code{y}.}

\item{k}{Degree of the piecewise polynomials that make up the trend
filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{The number of hyperparameter settings to test during
validation. Defaults to \code{nlambdas = 250}. The hyperparameter grid is
dynamically constructed to be representative of the full model space between
a single polynomial solution and an interpolating solution, with \code{nlambdas}
controlling the granularity of the grid.}

\item{nx_eval}{Integer. The length of the input grid that the optimized
trend filtering estimate is evaluated on; i.e. if nothing is passed to
\code{x_eval}, then it is defined as
\code{x_eval = seq(min(x), max(x), length = nx_eval)}.}

\item{x_eval}{(Optional) Overrides \code{nx_eval} if passed. A grid of inputs to
evaluate the optimized trend filtering estimate on.}

\item{optimization_params}{(Optional) A named list of optimization parameter
values to be passed to the trend filtering ADMM algorithm of
\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and Tibshirani (2016)}, which is implemented in
the \code{glmgen} R package. See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for full details. The default parameter choices will almost
always suffice, but when adjustments are necessary, no technical
understanding of the ADMM algorithm is needed in order to do so. The
following parameters may require some adjustments to ensure that your trend
filtering estimate has sufficiently converged:
\describe{
\item{\code{obj_tol}}{The objective tolerance that, together with \code{max_iter},
determines the ADMM algorithm's stopping criterion. The algorithm will stop
either (1) when the relative change in the objective function is less than
\code{obj_tol}; or (2) when the number of iterations has reached \code{max_iter}.
This argument defaults to \code{obj_tol = 1e-10}. Therefore, when necessary, the
precision of the approximate solution given by the ADMM algorithm can be
increased by decreasing \code{obj_tol} and/or increasing \code{max_iter}.}
\item{\code{max_iter}}{Maximum iterations allowed for the trend filtering
optimization. Defaults to \code{max_iter = length(y)}. See the
\code{n_iter} element of the \code{sure_trendfilter()} output for the actual number of
iterations the ADMM algorithm took, for every candidate hyperparameter value
in \code{lambdas}. If any of the elements of \code{n_iter} are equal to \code{max_iter},
the objective function's tolerance has not been reached and \code{max_iter} may
need to be increased.}
\item{\code{thinning}}{Logical. If \code{TRUE}, then the data are preprocessed so that
a smaller, better conditioned data set is used for fitting. When left \code{NULL}
(the default setting), the optimization will automatically detect whether
thinning should be applied (i.e. cases in which the numerical fitting
algorithm will struggle to converge). This preprocessing procedure is
controlled by the \code{x_tol} argument below.}
\item{\code{x_tol}}{Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then the data is thinned.
}}}
}
\value{
An object of class \code{\link[=sure_trendfilter]{sure_tf}}. This is a list
with the following elements:
\describe{
\item{x_eval}{Input grid used to evaluate the optimized trend filtering
estimate on.}
\item{tf_estimate}{Optimized trend filtering estimate, evaluated at \code{x_eval}.
}
\item{validation_method}{"SURE"}
\item{lambdas}{Vector of hyperparameter values evaluated in the grid search
(always returned in descending order).}
\item{edfs}{Vector of effective degrees of freedom for all trend filtering
estimators fit during validation.}
\item{generalization_errors}{Vector of SURE generalization error estimates,
corresponding to the descending-ordered \code{lambdas} vector.}
\item{lambda_min}{Hyperparameter value that minimizes the SURE generalization
error curve.}
\item{edf_min}{Effective degrees of freedom of the optimized trend
filtering estimator.}
\item{i_min}{Index of \code{lambdas} that minimizes the SURE error curve.}
\item{cost_functional}{The relative change in the cost functional values
between the ADMM algorithm's penultimate and final iterations, for
every hyperparameter choice.}
\item{n_iter}{The number of iterations taken by the ADMM algorithm along its
approximate solution path.
converge within the given tolerance, for each hyperparameter value. If many
of these are exactly equal to \code{max_iter}, then their solutions have not
converged with the tolerance specified by \code{obj_tol}. In which case, it is
often prudent to increase \code{max_iter}.}
\item{training_errors}{Mean-squared error between the observed outputs \code{y}
and the trend filtering estimate, for every hyperparameter choice.}
\item{optimisms}{SURE-estimated optimisms, i.e.
\code{optimisms = generalization_errors - training_errors}.}
\item{x}{Vector of observed inputs.}
\item{y}{Vector of observed outputs.}
\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal.}
\item{fitted_values}{Optimized trend filtering estimate, evaluated at the
observed inputs \code{x}.}
\item{residuals}{\code{residuals = y - fitted_values}}
\item{k}{Degree of the trend filtering estimator.}
\item{admm_params}{List of parameter settings for the trend filtering ADMM
algorithm, constructed by passing the \code{optimization_params} list to
\code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}.}
\item{thinning}{Logical. If \code{TRUE}, then the data were preprocessed such
that a reduced subset was passed to the trend filtering ADMM algorithm in
order to make for a more tractable/stable problem and solution.}
\item{x_scale, y_scale, data_scaled}{For internal use.}
}
}
\description{
\code{\link[=sure_trendfilter]{sure_trendfilter()}} optimizes the trend filtering hyperparameter via a
grid search over a vector of candidate hyperparameter values and selects
the value that minimizes an unbiased estimate of the model's generalization
mean-squared error (at the observed inputs). See details below for when you
should use \code{\link[=sure_trendfilter]{sure_trendfilter()}} vs. \code{\link[=cv_trendfilter]{cv_trendfilter()}}.
}
\details{
Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} vs.
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are shown in the table below.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in large consecutive chunks) is still considered to
be evenly sampled. When the inputs are evenly sampled on a transformed scale,
we recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the example below for a case when the
inputs are evenly sampled on the \code{log10(x)} scale.
}
\examples{
data(quasar_spectrum)
head(spec)

sure_tf <- sure_trendfilter(spec$log10_wavelength, spec$flux, spec$weights)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Stein's unbiased risk estimate}
\enumerate{
\item{Tibshirani and Wasserman (2015).
\href{http://www.stat.cmu.edu/~larry/=sml/stein.pdf}{Stein’s Unbiased Risk
Estimate}. \emph{36-702: Statistical Machine Learning course notes}
(Carnegie Mellon University).} \cr
\item{Efron (2014).
\href{https://www.tandfonline.com/doi/abs/10.1198/016214504000000692}{
The Estimation of Prediction Error: Covariance Penalties
and Cross-Validation}. \emph{Journal of the American Statistical
Association}. 99(467), p. 619-632.} \cr
\item{Stein (1981).
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-6/Estimation-of-the-Mean-of-a-Multivariate-Normal-Distribution/10.1214/aos/1176345632.full}{
Estimation of the Mean of a Multivariate Normal Distribution}.
\emph{The Annals of Statistics}. 9(6), p. 1135-1151.}}

\bold{Effective degrees of freedom for trend filtering}
\enumerate{
\item{Tibshirani and Taylor (2012)}.
\href{https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-2/Degrees-of-freedom-in-lasso-problems/10.1214/12-AOS1003.full}{
Degrees of freedom in lasso problems}. \emph{The Annals of Statistics},
40(2), p. 1198-1232.}
}
\seealso{
\code{\link[=cv_trendfilter]{cv_trendfilter()}}, \code{\link[=bootstrap_trendfilter]{bootstrap_trendfilter()}}
}
