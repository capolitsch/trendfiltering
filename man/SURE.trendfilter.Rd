% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SURE.trendfilter.R
\name{SURE.trendfilter}
\alias{SURE.trendfilter}
\title{Optimize the trend filtering hyperparameter by minimizing Stein's unbiased
risk estimate}
\usage{
SURE.trendfilter(
  x,
  y,
  weights,
  ngammas = 250L,
  gammas,
  x.eval = x,
  nx.eval,
  k = 2L,
  optimization.params = list(max_iter = 600L, obj_tol = 1e-10, thinning = NULL)
)
}
\arguments{
\item{x}{The vector of observed values of the input variable (a.k.a. the
predictor, covariate, explanatory variable, regressor, independent variable,
control variable, etc.)}

\item{y}{The vector of observed values of the output variable (a.k.a. the
response, target, outcome, regressand, dependent variable, etc.)}

\item{weights}{\strong{Must be passed.} A vector of weights for the
observed outputs, defined as the reciprocal of the variance of the error
distribution. That is, \code{weights = 1 / sigmas^2}, where \code{sigmas} is a vector
of standard errors of the uncertainty in the observed outputs. \code{weights}
should either have length equal to 1 (corresponding to an error distribution
with a constant variance) or length equal to \code{length(y)}
(i.e. heteroskedastic errors).}

\item{ngammas}{Integer. The number of trend filtering hyperparameter values
to run the grid search over. In this default case, the hyperparameter values
are automatically chosen by \code{SURE.trendfilter} and \code{ngammas} simply controls
the granularity of the grid.}

\item{gammas}{Overrides \code{ngammas} if passed. A vector of trend filtering
hyperparameter values to run the grid search over. It is advisable to let
the vector be equally-spaced in log-space and passed to \code{SURE.trendfilter}
in descending order. The function output will contain the sorted
hyperparameter vector regardless of the user-supplied ordering, and all
related output objects (e.g. the \code{errors} vector) will correspond to this
descending ordering. It's best to leave this
argument alone unless you know what you are doing.}

\item{x.eval}{A grid of inputs to evaluate the optimized trend filtering
estimate on. Defaults to the observed inputs, \code{x}.}

\item{nx.eval}{Integer. If passed, then \code{x.eval} is overridden with \cr
\code{x.eval = seq(min(x), max(x), length = nx.eval)}}

\item{k}{The degree of the trend filtering estimator. More precisely, with
the trend filtering estimator defined as a piecewise function of polynomials
smoothly connected at a set of "knots", \code{k} controls the degree of the
polynomials that build up the trend filtering estimator.
Defaults to \code{k = 2} (i.e. a piecewise quadratic estimate). Must be one of
\verb{k = 0,1,2,3}. However, \code{k = 3} is discouraged due to algorithmic
instability, and \code{k = 2} typically gives a visually indistinguishable
estimate anyway.}

\item{optimization.params}{A named list of parameters that contains all
parameter choices to be passed to the trend filtering ADMM algorithm
(\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and
Tibshirani 2016}). See the documentation for the \pkg{glmgen} function
\code{\link[glmgen]{trendfilter.control.list}} for full details.
No technical understanding of the ADMM algorithm is needed and the default
parameter choices will almost always suffice. However, the following
parameters may require some adjustments to ensure that your trend filtering
estimate has sufficiently converged:
\enumerate{
\item{\code{max_iter}}: Maximum iterations allowed for the trend filtering
convex optimization. Defaults to \code{max_iter = 600L}. Increase this if
the trend filtering estimate does not appear to have fully converged to a
reasonable estimate of the signal.
\item{\code{obj_tol}}: The tolerance used in the convex optimization stopping
criterion; when the relative change in the objective function is less than
this value, the algorithm terminates. Decrease this if the trend filtering
estimate does not appear to have fully converged to a reasonable estimate of
the signal.
\item{\code{thinning}}: Logical. If \code{TRUE}, then the data are
preprocessed so that a smaller, better conditioned data set is used for
fitting. When left \code{NULL} (the default choice), the optimization will
automatically detect whether thinning should be applied (i.e. cases in
which the numerical fitting algorithm will struggle to converge). This
preprocessing procedure is controlled by the \code{x_tol} argument below.
\item{\code{x_tol}}: Controls the automatic detection of when thinning should be
applied to the data. If we make bins
of size \code{x_tol} and find at least two elements of \code{x} that fall into the
same bin, then we thin the data.}}
}
\value{
An object of class 'SURE.trendfilter'. This is a list with the
following elements:
\item{x.eval}{The grid of inputs the optimized trend filtering estimate was
evaluated on.}
\item{tf.estimate}{The optimized trend filtering estimate of the signal,
evaluated on \code{x.eval}.}
\item{validation.method}{"SURE"}
\item{gammas}{Vector of hyperparameter values tested during validation
(always returned in descending order).}
\item{errors}{Vector of SURE error estimates corresponding to the
\emph{descending} set of gamma values tested during validation.}
\item{gamma.min}{Hyperparameter value that minimizes the SURE error curve.}
\item{edfs}{Vector of effective degrees of freedom for all trend filtering
estimators fit during validation.}
\item{edf.min}{The effective degrees of freedom of the optimally-tuned trend
filtering estimator.}
\item{i.min}{The index of \code{gammas} (descending order) that minimizes
the SURE error curve.}
\item{x}{The vector of the observed inputs.}
\item{y}{The vector of the observed outputs.}
\item{weights}{A vector of weights for the observed outputs. These are
defined as \code{weights = 1 / sigmas^2}, where \code{sigmas} is a vector of
standard errors of the uncertainty in the observed outputs.}
\item{fitted.values}{The optimized trend filtering estimate of the signal,
evaluated at the observed inputs \code{x}.}
\item{residuals}{\code{residuals = y - fitted.values}}
\item{k}{The degree of the trend filtering estimator.}
\item{optimization.params}{A list of parameters that control the trend
filtering convex optimization.}
\item{n.iter}{Vector of the number of iterations needed for the ADMM
algorithm to converge within the given tolerance, for each hyperparameter
value. If many of these are exactly equal to \code{max_iter}, then their
solutions have not converged with the tolerance specified by \code{obj_tol}.
In which case, it is often prudent to increase \code{max_iter}.}
\item{thinning}{Logical. If \code{TRUE}, then the data are preprocessed so
that a smaller, better conditioned data set is used for fitting.}
\item{x.scale, y.scale, data.scaled}{For internal use}
}
\description{
\code{SURE.trendfilter} optimizes the trend
filtering hyperparameter by running a grid search over the vector, \code{gammas},
of candidate hyperparameter values, and then selects the value that minimizes
an unbiased estimate of the model's generalization error. The full
generalization error curve and the optimized trend filtering estimate of the
signal are then returned within a list that also includes useful ancillary
information.
}
\details{
\loadmathjax \code{SURE.trendfilter} estimates the fixed-input
mean-squared error of a trend filtering estimator by computing Stein's
unbiased risk estimate (a.k.a. SURE) over a grid of hyperparameter
values, which should typically be equally-spaced in log-space. The full error
curve and the optimized trend filtering estimate are returned within a
list that also includes useful ancillary information.

Given the choice of \mjeqn{k}{ascii}, the hyperparameter
\mjeqn{\gamma}{ascii} is used to tune the complexity (i.e. the wiggliness)
of the trend filtering estimate by weighting the tradeoff between the
complexity of the estimate and the size of the squared residuals. Obtaining
an accurate estimate is therefore intrinsically tied to finding an optimal
choice of \mjeqn{\gamma}{ascii}. The selection of \mjeqn{\gamma}{ascii}
is typically done by minimizing an estimate of the mean-squared prediction
error (MSPE) of the trend filtering estimator. Here, there are two different
notions of error to consider, namely, \emph{fixed-input} error
and \emph{random-input} error. As the names suggest, the distinction between
which type of error to consider is made based on how the inputs are sampled.
As a general rule-of-thumb, we recommend optimizing with respect to
fixed-input error when the inputs are regularly-sampled and optimizing with
respect to random-input error on irregularly-sampled data.

Recall the DGP stated in (link). Further,
let \mjeqn{\sigma_{i}^{2} = \text{Var}(\epsilon_{i}).}{ascii}
The fixed-input MSPE is given by
\mjdeqn{R(\gamma) = \frac{1}{n}\sum_{i=1}^{n}\;\mathbb{E}\left[\left(f(t_{i}) - \widehat{f}_{0}(t_{i};\gamma)\right)^2\;|\;t_{1},\dots,t_{n}\right]}{ascii}
and the random-input MSPE is given by
\mjdeqn{\widetilde{R}(\gamma) = \mathbb{E}\left[\left(f(t) - \widehat{f}_{0}(t;\gamma)\right)^{2}\right],}{ascii}
where, in the latter, \mjeqn{t}{ascii} is considered to be a random component of the DGP
with a marginal probability density \mjeqn{p_t(t)}{ascii} supported on the observed input
interval. In each case, the theoretically optimal choice of \mjeqn{\gamma}{ascii} is
defined as the minimizer of the respective choice of error. Empirically, we
estimate the theoretically optimal choice of \mjeqn{\gamma}{ascii} by minimizing an
estimate of (link) or (link). For fixed-input
error we recommend Stein's unbiased risk estimate
(SURE; (link)) and for random-input error we recommend
\mjeqn{K}{ascii}-fold cross validation with \mjeqn{K = 10}{ascii}. We elaborate on SURE here and refer
the reader to (link) for \mjeqn{K}{ascii}-fold cross validation.

The SURE formula provides an unbiased estimate of the fixed-input MSPE of a
statistical estimator:
\mjdeqn{\widehat{R}_{0}(\gamma) = \frac{1}{n}\sum_{i=1}^{n}\big(f(t_i) - \widehat{f}_0(t_i; \gamma)\big)^2 + \frac{2\overline{\sigma}^{2}\text{df}(\widehat{f}_0)}{n},}{ascii}
where \mjeqn{\overline{\sigma}^{2} = n^{-1}\sum_{i=1}^{n} \sigma_i^2}{ascii} and
\mjeqn{\text{df}(\widehat{f}_0)}{ascii} is defined above. A formula for the
effective degrees of freedom of the trend filtering estimator is available
via the generalized lasso results of (link); namely,
\mjdeqn{\text{df}(\widehat{f}_0) = \mathbb{E}\left[\text{number of knots in}\;\widehat{f}_0\right] + k + 1.}{ascii}
We then obtain our hyperparameter estimate \mjeqn{\widehat{\gamma}}{ascii} by minimizing the
following plug-in estimate for (link):
\mjdeqn{\widehat{R}(\gamma) = \frac{1}{n}\sum_{i=1}^{n}\big(f(t_i) - \widehat{f}_0(t_i; \gamma)\big)^2 + \frac{2\widehat{\overline{\sigma}}^{2}\widehat{\text{df}}(\widehat{f}_0)}{n},}{ascii}
where \mjeqn{\widehat{\text{df}}}{ascii} is the estimate for the effective degrees of
freedom that is obtained by replacing the expectation in (link) with
the observed number of knots, and \mjeqn{\widehat{\overline{\sigma}}^2}{ascii} is an
estimate of \mjeqn{\overline{\sigma}^2}{ascii}. If a reliable estimate of
\mjeqn{\overline{\sigma}^2}{ascii} is not available \emph{a priori}, a data-driven
estimate can be constructed (see, e.g., (link)).
}
\references{
\enumerate{
\item{Politsch et al. (2020a). Trend filtering – I. A modern
statistical tool for time-domain astronomy and astronomical spectroscopy.
\emph{Monthly Notices of the Royal Astronomical Society}, 492(3),
p. 4005-4018.
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{\strong{\link{Link}}}}
\item{Politsch et al. (2020b). Trend Filtering – II. Denoising
astronomical signals with varying degrees of smoothness. \emph{Monthly
Notices of the Royal Astronomical Society}, 492(3), p. 4019-4032.
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{\strong{\link{Link}}}}}
}
\seealso{
\code{\link{bootstrap.trendfilter}}
}
\author{
Collin A. Politsch, Ph.D., \email{collinpolitsch@gmail.com}
}
