% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SURE.trendfilter.R
\name{SURE.trendfilter}
\alias{SURE.trendfilter}
\title{Optimize the trend filtering hyperparameter by minimizing Stein's unbiased
risk estimate}
\usage{
SURE.trendfilter(
  x,
  y,
  weights,
  ngammas = 250L,
  gammas,
  x.eval = x,
  nx.eval,
  k = 2L,
  optimization.params = list(max_iter = 600L, obj_tol = 1e-10, thinning = NULL),
  ...
)
}
\arguments{
\item{x}{The vector of observed values of the input variable (a.k.a. the
predictor, covariate, explanatory variable, regressor, independent variable,
control variable, etc.)}

\item{y}{The vector of observed values of the output variable (a.k.a. the
response, target, outcome, regressand, dependent variable, etc.).}

\item{weights}{\strong{Must be passed.} A vector of weights for the observed
outputs, defined as the reciprocal of the variance of the error distribution.
That is, \code{weights = 1 / sigmas^2}, where \code{sigmas} is a vector of standard
errors of the uncertainty in the observed outputs. \code{weights} should either
have length equal to 1 (corresponding to an error distribution with a
constant variance) or length equal to \code{length(y)} (i.e. heteroskedastic
errors).}

\item{ngammas}{Integer. The number of trend filtering hyperparameter values
to run the grid search over. In this default case, the hyperparameter values
are automatically chosen by \code{SURE.trendfilter} and \code{ngammas} simply controls
the granularity of the grid.}

\item{gammas}{Overrides \code{ngammas} if passed. A vector of trend filtering
hyperparameter values to run the grid search over. It is advisable to let
the vector be equally-spaced in log-space and passed to \code{SURE.trendfilter}
in descending order. The function output will contain the sorted
hyperparameter vector regardless of the user-supplied ordering, and all
related output objects (e.g. the \code{errors} vector) will correspond to this
descending ordering. It's best to leave this argument alone unless you know
what you are doing.}

\item{x.eval}{A grid of inputs to evaluate the optimized trend filtering
estimate on. Defaults to the observed inputs, \code{x}.}

\item{nx.eval}{Integer. If passed, overrides \code{x.eval} with
\code{x.eval = seq(min(x), max(x), length = nx.eval)}}

\item{k}{The degree of the trend filtering estimator. More precisely, with
the trend filtering estimator defined as a piecewise function of polynomials
smoothly connected at a set of "knots", \code{k} controls the degree of the
polynomials that build up the trend filtering estimator. Defaults to \code{k = 2}
(i.e. a piecewise quadratic estimate). Must be one of \verb{k = 0,1,2,3}. However,
\code{k = 3} is discouraged due to algorithmic instability, and \code{k = 2} typically
gives a visually indistinguishable estimate anyway.}

\item{optimization.params}{A named list of parameters that contains all
parameter choices to be passed to the trend filtering ADMM algorithm
(\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and
Tibshirani 2016}). See the \code{\link[glmgen]{trendfilter.control.list}}
documentation for full details. No technical understanding of the ADMM
algorithm is needed and the default parameter choices will almost always
suffice. However, the following parameters may require some adjustments to
ensure that your trend filtering estimate has sufficiently converged:
\enumerate{
\item{\code{max_iter}}: Maximum iterations allowed for the trend filtering convex
optimization. Defaults to \code{max_iter = 600L}. Increase this if the trend
filtering estimate does not appear to have fully converged to a reasonable
estimate of the signal.
\item{\code{obj_tol}}: The tolerance used in the convex optimization stopping
criterion; when the relative change in the objective function is less than
this value, the algorithm terminates. Decrease this if the trend filtering
estimate does not appear to have fully converged to a reasonable estimate of
the signal.
\item{\code{thinning}}: Logical. If \code{TRUE}, then the data are preprocessed so that
a smaller, better conditioned data set is used for fitting. When left \code{NULL}
(the default setting), the optimization will automatically detect whether
thinning should be applied (i.e. cases in which the numerical fitting
algorithm will struggle to converge). This preprocessing procedure is
controlled by the \code{x_tol} argument below.
\item{\code{x_tol}}: Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then we thin the data.}}

\item{...}{Additional named arguments to be passed to
\code{\link[glmgen]{trendfilter.control.list}}.}
}
\value{
An object of class 'SURE.trendfilter'. This is a list with the
following elements:
\item{x.eval}{The grid of inputs the optimized trend filtering estimate was
evaluated on.}
\item{tf.estimate}{The optimized trend filtering estimate of the signal,
evaluated on \code{x.eval}.}
\item{validation.method}{"SURE"}
\item{gammas}{Vector of hyperparameter values tested during validation
(always returned in descending order).}
\item{errors}{Vector of SURE error estimates corresponding to the
\strong{descending} set of gamma values tested during validation.}
\item{gamma.min}{Hyperparameter value that minimizes the SURE error curve.}
\item{edfs}{Vector of effective degrees of freedom for all trend filtering
estimators fit during validation.}
\item{edf.min}{The effective degrees of freedom of the optimally-tuned trend
filtering estimator.}
\item{i.min}{The index of \code{gammas} (descending order) that minimizes the
SURE error curve.}
\item{x}{The vector of the observed inputs.}
\item{y}{The vector of the observed outputs.}
\item{weights}{A vector of weights for the observed outputs. These are
defined as \code{weights = 1 / sigmas^2}, where \code{sigmas} is a vector of
standard errors of the uncertainty in the observed outputs.}
\item{fitted.values}{The optimized trend filtering estimate of the signal,
evaluated at the observed inputs \code{x}.}
\item{residuals}{\code{residuals = y - fitted.values}}
\item{k}{The degree of the trend filtering estimator.}
\item{optimization.params}{A list of parameters that control the trend
filtering convex optimization.}
\item{n.iter}{Vector of the number of iterations needed for the ADMM
algorithm to converge within the given tolerance, for each hyperparameter
value. If many of these are exactly equal to \code{max_iter}, then their
solutions have not converged with the tolerance specified by \code{obj_tol}.
In which case, it is often prudent to increase \code{max_iter}.}
\item{thinning}{Logical. If \code{TRUE}, then the data are preprocessed so that a
smaller, better conditioned data set is used for fitting.}
\item{x.scale, y.scale, data.scaled}{For internal use.}
}
\description{
\code{SURE.trendfilter} optimizes the trend filtering hyperparameter via a grid
search over the vector, \code{gammas}, of candidate hyperparameter values, and
then selects the value that minimizes an unbiased estimate of the model's
generalization error. The full generalization error curve and the optimized
trend filtering estimate are then returned within a list that also includes
a detailed summary of the analysis.
}
\details{
\loadmathjax Recall the DGP stated in (link). Further, let
\mjeqn{\sigma_{i}^{2} = \text{Var}(\epsilon_{i}).}{ascii}
The fixed-input MSPE is given by
\mjdeqn{R(\gamma) = \frac{1}{n}\sum_{i=1}^{n}\;\mathbb{E}\left[\left(f(t_{i}) - \widehat{f}\emph{{0}(t}{i};\gamma)\right)^2\;|\;t_{1},\dots,t_{n}\right]}{ascii}
and the random-input MSPE is given by
\mjdeqn{\widetilde{R}(\gamma) = \mathbb{E}\left[\left(f(t) - \widehat{f}_{0}(t;\gamma)\right)^{2}\right],}{ascii}
where, in the latter, \mjeqn{t}{ascii} is considered to be a random
component of the DGP with a marginal probability density
\mjeqn{p_t(t)}{ascii} supported on the observed input interval. In each case,
the theoretically optimal choice of \mjeqn{\gamma}{ascii} is defined as the
minimizer of the respective choice of error.

The SURE formula provides an unbiased estimate of the fixed-input MSPE of a
statistical estimator:
\mjdeqn{\widehat{R}\emph{0(\gamma) = \frac{1}{n}\sum}{i=1}^{n}\big(f(t_i) - \widehat{f}_0(t_i; \gamma)\big)^2 + \frac{2\overline{\sigma}^{2}\text{df}(\widehat{f}\emph{0)}{n},}{ascii}
where \mjeqn{\overline{\sigma}^{2} = n^{-1}\sum}{i=1}^{n} \sigma_i^2}{ascii}
and \mjeqn{\text{df}(\widehat{f}_0)}{ascii} is defined above. A formula for
the effective degrees of freedom of the trend filtering estimator is
available via the generalized lasso results of (link); namely,
\mjdeqn{\text{df}(\widehat{f}_0) = \mathbb{E}\left[\text{number of knots in}\;\widehat{f}\emph{0\right] + k + 1.}{ascii}
We then obtain our hyperparameter estimate \mjeqn{\widehat{\gamma}}{ascii}
by minimizing the following plug-in estimate for (link):
\mjdeqn{\widehat{R}(\gamma) = \frac{1}{n}\sum}{i=1}^{n}\big(f(t_i) - \widehat{f}_0(t_i; \gamma)\big)^2 + \frac{2\widehat{\overline{\sigma}}^{2}\widehat{\text{df}}(\widehat{f}_0)}{n},}{ascii}
where \mjeqn{\widehat{\text{df}}}{ascii} is the estimate for the effective
degrees of freedom that is obtained by replacing the expectation in (link)
with the observed number of knots, and
\mjeqn{\widehat{\overline{\sigma}}^2}{ascii} is an estimate of
\mjeqn{\overline{\sigma}^2}{ascii}.
}
\references{
\strong{Companion references}
\enumerate{
\item{\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Politsch et al. (2020a). Trend filtering – I. A modern statistical tool
for time-domain astronomy and astronomical spectroscopy. \emph{Monthly
Notices of the Royal Astronomical Society}, 492(3), p. 4005-4018.}} \cr
\item{\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Politsch et al. (2020b). Trend Filtering – II. Denoising astronomical
signals with varying degrees of smoothness. \emph{Monthly Notices of the
Royal Astronomical Society}, 492(3), p. 4019-4032.}}}

\strong{Trend filtering theory}
\enumerate{
\item{\href{https://projecteuclid.org/euclid.aos/1395234979}{
Tibshirani (2014). Adaptive piecewise polynomial estimation via trend
filtering. \emph{The Annals of Statistics}. 42(1), p. 285-323.}} \cr
\item{\href{https://arxiv.org/abs/2003.03886}{
Tibshirani (2020). Divided Differences, Falling Factorials, and
Discrete Splines: Another Look at Trend Filtering and Related Problems.}}}

\strong{Stein's unbiased risk estimate}
\enumerate{
\item{\href{http://www.stat.cmu.edu/~larry/=sml/stein.pdf}{
Tibshirani and Wasserman (2015). Stein’s Unbiased Risk Estimate.
\emph{36-702: Statistical Machine Learning course notes} (Carnegie Mellon
University).}} \cr
\item{\href{https://www.tandfonline.com/doi/abs/10.1198/016214504000000692}{
Efron (2014). The Estimation of Prediction Error: Covariance Penalties
and Cross-Validation. \emph{Journal of the American Statistical Association}.
99(467), p. 619-632.}} \cr
\item{\href{https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-6/Estimation-of-the-Mean-of-a-Multivariate-Normal-Distribution/10.1214/aos/1176345632.full}{
Stein (1981). Estimation of the Mean of a Multivariate Normal
Distribution. \emph{The Annals of Statistics}. 9(6), p. 1135-1151.}}}

\strong{Effective degrees of freedom for trend filtering}
\enumerate{
\item{\href{https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-2/Degrees-of-freedom-in-lasso-problems/10.1214/12-AOS1003.full}{
Tibshirani and Taylor (2012). Degrees of freedom in lasso problems.
\emph{The Annals of Statistics}, 40(2), p. 1198-1232.}}}
}
\seealso{
\code{\link{cv.trendfilter}}, \code{\link{bootstrap.trendfilter}}
}
\author{
\subsection{\cr
\strong{Collin A. Politsch, Ph.D.}}{

Email: collinpolitsch@gmail.com \cr
Website: \href{https://collinpolitsch.com/}{collinpolitsch.com} \cr
GitHub: \href{https://github.com/capolitsch/}{github.com/capolitsch} \cr \cr
}
}
