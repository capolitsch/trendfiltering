% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv.trendfilter.R
\name{cv.trendfilter}
\alias{cv.trendfilter}
\title{Optimize the trend filtering hyperparameter by V-fold cross validation}
\usage{
cv.trendfilter(
  x,
  y,
  weights,
  k = 2L,
  nlambdas = 250L,
  lambdas,
  V = 10L,
  lambda.choice = c("lambda.min", "lambda.1se"),
  validation.functional = "WMAE",
  x.eval,
  nx.eval = 1500L,
  mc.cores = detectCores(),
  optimization.params = list(max_iter = 600L, obj_tol = 1e-10)
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal. \code{weights} can be
passed as a scalar when the noise is expected to have equal variance for all
observations. Otherwise, \code{weights} must have the same length as \code{x} and \code{y}.}

\item{k}{Degree of the piecewise polynomials that make up the trend
filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{The number of hyperparameter settings to test during
validation. When nothing is passed to \code{lambdas} (highly recommended for
general use), the grid is automatically constructed by \code{SURE.trendfilter},
with \code{nlambdas} controlling the granularity of the grid.}

\item{lambdas}{(Optional) Overrides \code{nlambdas} if passed. The vector of trend
filtering hyperparameter values for the grid search. Use of this argument is
discouraged unless you know what you are doing.}

\item{V}{Number of folds the data are partitioned into for the V-fold cross
validation. Defaults to \code{V = 10}.}

\item{lambda.choice}{One of \code{c("lambda.min","lambda.1se")}. The choice
of hyperparameter that is used for optimized trend filtering estimate.
Defaults to \code{lambda.min}.
\itemize{
\item{\code{lambda.min}}: The hyperparameter value that minimizes the cross
validation error curve.
\item{\code{lambda.1se}}: The largest hyperparameter value with a cross
validation error within 1 standard error of the minimum cross validation
error. This choice therefore favors simpler (i.e. smoother) trend filtering
estimates. The motivation here is essentially Occam's razor: the two models
yield results that are quantitatively very close, so we favor the simpler
model. See Section 7.10 of
\href{https://web.stanford.edu/~hastie/Papers/ESLII.pdf}{Hastie, Tibshirani, and Friedman (2009)}
for more details on the ``one-standard-error rule''.}}

\item{validation.functional}{Loss functional to optimize during cross
validation. Some common choices can be used by passing an appropriate string
--- one of \code{c("MAE","MSE","WMAE","WMSE")}, i.e. mean-absolute deviations
error, mean-squared error, and their weighted counterparts. Defaults to
\code{validation.functional = "WMAE"}.

Custom validation loss functionals can be used by instead passing a function
to \code{validation.functional}. The function should take three vector arguments
--- \code{y}, \code{tf.estimate}, and \code{weights} --- and return a single scalar value
for the validation loss. For example, \code{validation.functional = "WMAE"} is
equivalent to passing the following function:\if{html}{\out{<div class="sourceCode r">}}\preformatted{function(tf.estimate, y, weights)\{
  sum(abs(tf.estimate - y) * sqrt(weights) / sum(sqrt(weights)))
\}
}\if{html}{\out{</div>}}}

\item{x.eval}{(Optional) A grid of inputs to evaluate the optimized trend
filtering estimate on. May be ignored, in which case the grid is determined
by \code{nx.eval}.}

\item{nx.eval}{Integer. If nothing is passed to \code{x.eval}, then it is defined
as \code{x.eval = seq(min(x), max(x), length = nx.eval)}.}

\item{mc.cores}{Parallel computing: The number of cores to utilize. Defaults
to the number of cores detected on the machine.}

\item{optimization.params}{A named list of parameter choices to be passed to
the trend filtering ADMM algorithm
(\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and
Tibshirani 2016}). See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for full details. No technical understanding of the ADMM
algorithm is needed and the default parameter choices will almost always
suffice. However, the following parameters may require some adjustments to
ensure that your trend filtering estimate has sufficiently converged:
\enumerate{
\item{\code{max_iter}}: Maximum iterations allowed for the trend filtering convex
optimization. Defaults to \code{max_iter = 600L}. See the \code{n.iter} element
of the function output for the actual number of iterations taken for every
hyperparameter choice in \code{lambdas}. If any of the elements of \code{n.iter} are
equal to \code{max_iter}, the objective function's tolerance has not been
achieved and \code{max_iter} may need to be increased.
\item{\code{obj_tol}}: The tolerance used in the convex optimization stopping
criterion; when the relative change in the objective function is less than
this value, the algorithm terminates. Thus, decreasing this setting will
increase the precision of the solution returned by the optimization. Defaults
to \code{obj_tol = 1e-10}. If the returned trend filtering estimate does not
appear to have fully converged to a reasonable estimate of the signal, this
issue can be resolve by some combination of decreasing \code{obj_tol} and
increasing \code{max_iter}.
\item{\code{thinning}}: Logical. If \code{TRUE}, then the data are preprocessed so that
a smaller, better conditioned data set is used for fitting. When left \code{NULL}
(the default setting), the optimization will automatically detect whether
thinning should be applied (i.e. cases in which the numerical fitting
algorithm will struggle to converge). This preprocessing procedure is
controlled by the \code{x_tol} argument below.
\item{\code{x_tol}}: Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then we thin the data.}}
}
\value{
An object of class 'cv.trendfilter'. This is a list with the
following elements:
\item{x.eval}{Input grid used to evaluate the optimized trend filtering
estimate on.}
\item{tf.estimate}{Optimized trend filtering estimate, evaluated at \code{x.eval}.}
\item{validation.method}{\code{paste0(V,"-fold CV")}}
\item{V}{The number of folds the data are split into for the V-fold cross
validation.}
\item{validation.functional}{Type of error that validation was performed on.
Either one of \code{c("WMAE","WMSE","MAE","MSE")} or a custom function passed by
the user.}
\item{lambdas}{Vector of hyperparameter values evaluated in the grid search
(always returned in descending order).}
\item{edfs}{Vector of effective degrees of freedom for all trend filtering
estimators fit during validation.}
\item{generalization.errors}{Vector of cross validation estimates of the
trend filtering generalization error, for each hyperparameter value
(ordered corresponding to the descending-ordered \code{lambdas} vector).}
\item{se.errors}{The standard errors of the cross validation errors.
These are particularly useful for implementing the ``1-standard-error rule''.}
\item{lambda.min}{Hyperparameter value that minimizes the cross validation
generalization error curve.}
\item{lambda.1se}{Largest hyperparameter value that is within one standard
error of the minimum hyperparameter's cross validation error.}
\item{lambda.choice}{One of \code{c("lambda.min","lambda.1se")}. The choice
of hyperparameter that is used for the returned trend filtering estimate
evaluation `tf.estimate`.}
\item{i.min}{Index of `lambdas` that minimizes the cross validation error.}
\item{i.1se}{Index of `lambdas` that gives the largest hyperparameter
value that has a cross validation error within 1 standard error of the
minimum of the cross validation error curves.}
\item{edf.min}{Effective degrees of freedom of the optimized trend
filtering estimator.}
\item{edf.1se}{Effective degrees of freedom of the 1-stand-error rule
trend filtering estimator.}
\item{n.iter}{The number of iterations needed for the ADMM algorithm to
converge within the given tolerance, for each hyperparameter value. If many
of these are exactly equal to `max_iter`, then their solutions have not
converged with the tolerance specified by `obj_tol`. In which case, it is
often prudent to increase `max_iter`.}
\item{x}{Vector of observed inputs.}
\item{y}{Vector of observed outputs.}
\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal.}
\item{fitted.values}{Optimized trend filtering estimate, evaluated at the
observed inputs `x`.}
\item{residuals}{`residuals = y - fitted.values`}
\item{k}{Degree of the trend filtering estimator.}
\item{ADMM.params}{List of parameter settings for the trend filtering ADMM
algorithm, constructed by passing the `optimization.params` list to
\code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}.}
\item{thinning}{Logical. If `TRUE`, then the data are preprocessed so that a
smaller, better conditioned data set is used for fitting.}
\item{x.scale, y.scale, data.scaled}{For internal use.}
}
\description{
\code{cv.trendfilter} optimizes the trend filtering hyperparameter via V-fold
cross validation on a grid of candidate hyperparameter settings and selects
the value that minimizes a user-specified loss metric. See details for when
to use \code{\link{SURE.trendfilter}} vs. \code{cv.trendfilter}.
}
\details{
\loadmathjax Our recommendations for when to use
\code{\link{cv.trendfilter}} vs. \code{SURE.trendfilter}, as well as each of the
available settings for \code{bootstrap.algorithm} are shown in the table below.
The corresponding settings that should be used when constructing bootstrap
variability bands with \code{bootstrap.trendfilter} are also shown.

A regularly-sampled data set with some discarded pixels (either sporadically
or in large consecutive chunks) is still considered regularly sampled. When
the inputs are regularly sampled on a transformed scale, we recommend
transforming to that scale and carrying out the full trend filtering analysis
on that scale. See the example below for a case when the inputs are evenly
sampled on the \code{log10(x)} scale.\tabular{lrr}{
   Scenario \tab Hyperparameter optimization \tab \code{bootstrap.algorithm} \cr
   \code{x} is irregularly sampled \tab Use \code{cv.trendfilter} \tab "nonparametric" \cr
   \code{x} is regularly sampled and \code{weights} are not available \tab Use \code{cv.trendfilter} \tab "wild" \cr
   \code{x} is regularly sampled and \code{weights} are available \tab Use \code{SURE.trendfilter} \tab "parametric" \cr
}


The formal definitions of the common validation loss functionals available
via the options \code{validation.functional = c("WMAE","WMSE","MAE","MSE")} are
stated below.

\mjsdeqn{WMAE(\lambda) = \sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \lambda)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
\mjsdeqn{WMSE(\lambda) = \sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \lambda)|^2\frac{w_i}{\sum_jw_j}}
\mjsdeqn{MAE(\lambda) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \lambda)|}
\mjsdeqn{MSE(\lambda) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \lambda)|^2}
where \mjseqn{w_i} is the \mjseqn{i}th element of the \code{weights} vector.

If constant weights are passed, or if nothing is passed, then the weighted
and unweighted counterparts are equivalent. \cr

Briefly stated, weighting helps combat heteroskedasticity (varying levels
of uncertainty in the output measurements) and absolute error is less
sensitive to outliers than squared error.
}
\examples{
data(eclipsing_binary)
head(EB)
# |      phase|      flux|  std.err|
# |----------:|---------:|--------:|
# | -0.4986308| 0.9384845| 0.010160|
# | -0.4978067| 0.9295757| 0.010162|
# | -0.4957892| 0.9438493| 0.010162|

opt <- cv.trendfilter(EB$phase, EB$flux, 1 / EB$std.err^2,
  validation.functional = "MAE",
  optimization.params = list(max_iter = 5e3, obj_tol = 1e-6, thinning = T)
)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Cross validation}
\enumerate{
\item{Hastie, Tibshirani, and Friedman (2009).
\href{https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf}{
The Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd edition. Springer Series in Statistics. (See Sections 7.10
and 7.12)} \cr
\item{James, Witten, Hastie, and Tibshirani (2013).
\href{https://www.statlearning.com/}{An Introduction to Statistical Learning:
with Applications in R}. Springer. (See Section 5.1)} \cr
\item{Tibshirani (2013).
\href{https://www.stat.cmu.edu/~ryantibs/datamining/lectures/19-val2.pdf}{
Model selection and validation 2: Model assessment, more cross-validation}.
\emph{36-462: Data Mining course notes} (Carnegie Mellon University).}}
}
\seealso{
\code{\link{SURE.trendfilter}}, \code{\link{bootstrap.trendfilter}}
}
\author{
\emph{\bold{Collin A. Politsch, Ph.D.}} \cr
Email: collinpolitsch@gmail.com \cr
Website: \href{https://collinpolitsch.com/}{collinpolitsch.com} \cr
GitHub: \href{https://github.com/capolitsch/}{github.com/capolitsch} \cr \cr
}
