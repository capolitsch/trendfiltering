% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv.trendfilter.R
\name{cv.trendfilter}
\alias{cv.trendfilter}
\title{Optimize the trend filtering hyperparameter by V-fold cross validation}
\usage{
cv.trendfilter(
  x,
  y,
  weights,
  k = 2L,
  ngammas = 250L,
  gammas,
  V = 10L,
  gamma.choice = c("gamma.min", "gamma.1se"),
  validation.error.type = c("WMAE", "WMSE", "MAE", "MSE"),
  x.eval = x,
  nx.eval,
  optimization.params = list(max_iter = 600L, obj_tol = 1e-10),
  mc.cores = detectCores(),
  ...
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the error distribution. \code{weights} can be passed as a scalar when
the errors are believed to have equal variance for all observations.
Otherwise, \code{weights} must have the same length as \code{x} and \code{y}.}

\item{k}{The degree of the trend filtering estimator. More precisely, with
the trend filtering estimator defined as a piecewise function of polynomials
smoothly connected at a set of "knots", \code{k} controls the degree of the
polynomials that build up the trend filtering estimator. Defaults to \code{k = 2}
(i.e. a piecewise quadratic estimate). Must be one of \verb{k = 0,1,2,3}. However,
\code{k = 3} is discouraged due to algorithmic instability, and \code{k = 2} typically
gives a visually indistinguishable estimate anyway.}

\item{ngammas}{Integer. The number of trend filtering hyperparameter values
to run the grid search over. In this default case, the hyperparameter values
are automatically chosen by \code{SURE.trendfilter} and \code{ngammas} simply controls
the granularity of the grid.}

\item{gammas}{Overrides \code{ngammas} if passed. A vector of trend filtering
hyperparameter values to run the grid search over. It is advisable to let the
vector be equally-spaced in log-space and passed to \code{SURE.trendfilter} in
descending order. The function output will contain the sorted hyperparameter
vector regardless of the user-supplied ordering, and all related output
objects (e.g. the \code{errors} vector) will correspond to this descending
ordering. It's best to leave this argument alone unless you know what you
are doing.}

\item{V}{The number of folds the data are split into for the V-fold cross
validation. Defaults to \code{V = 10} (recommended).}

\item{gamma.choice}{One of \code{c("gamma.min","gamma.1se")}. The choice
of hyperparameter that is used for optimized trend filtering estimate.
\itemize{
\item{\code{gamma.min}}: The hyperparameter value that minimizes the cross
validation error curve.
\item{\code{gamma.1se}}: The largest hyperparameter value with a cross
validation error within 1 standard error of the minimum cross validation
error. This choice therefore favors simpler (i.e. smoother) trend filtering
estimates. The motivation here is essentially Occam's razor: the two models
yield results that are quantitatively very close, so we favor the simpler
model.}}

\item{validation.error.type}{Type of error to optimize during cross
validation. One of \code{c("WMAE","WMSE","MAE","MSE")}, i.e. mean-absolute
deviations error, mean-squared error, and their weighted counterparts.
If \code{weights = NULL}, then the weighted and unweighted counterparts are
equivalent. In short, weighting helps combat heteroskedasticity and absolute
error decreases sensitivity to outliers. Defaults to \code{"WMAE"}.}

\item{x.eval}{A grid of inputs to evaluate the optimized trend filtering
estimate on. Defaults to the observed inputs, \code{x}.}

\item{nx.eval}{Integer. If passed, overrides \code{x.eval} with
\code{seq(min(x), max(x), length = nx.eval)}.}

\item{optimization.params}{A named list of parameters that contains all
parameter choices to be passed to the trend filtering ADMM algorithm
(\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and
Tibshirani 2016}). See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for full details.
No technical understanding of the ADMM algorithm is needed and the default
parameter choices will almost always suffice. However, the following
parameters may require some adjustments to ensure that your trend filtering
estimate has sufficiently converged:
\enumerate{
\item{\code{max_iter}}: Maximum iterations allowed for the trend filtering
convex optimization. Defaults to \code{max_iter = 600L}. Increase this if
the trend filtering estimate does not appear to have fully converged to a
reasonable estimate of the signal.
\item{\code{obj_tol}}: The tolerance used in the convex optimization stopping
criterion; when the relative change in the objective function is less than
this value, the algorithm terminates. Decrease this if the trend filtering
estimate does not appear to have fully converged to a reasonable estimate of
the signal.
\item{\code{thinning}}: Logical. If \code{TRUE}, then the data are preprocessed so that
a smaller, better conditioned data set is used for fitting. When left \code{NULL}
(the default setting), the optimization will automatically detect whether
thinning should be applied (i.e. cases in which the numerical fitting
algorithm will struggle to converge). This preprocessing procedure is
controlled by the \code{x_tol} argument below.
\item{\code{x_tol}}: Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then we thin the data.
}}

\item{mc.cores}{Parallel computing: The number of cores to utilize. Defaults
to the number of cores detected.}

\item{...}{Additional named arguments to be passed to
\code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}.}
}
\value{
An object of class 'cv.trendfilter'. This is a list with the
following elements:
\item{x.eval}{The grid of inputs the optimized trend filtering estimate was
evaluated on.}
\item{tf.estimate}{The optimized trend filtering estimate of the signal,
evaluated on \code{x.eval}.}
\item{validation.method}{\code{paste0(V,"-fold CV")}}
\item{V}{The number of folds the data are split into for the V-fold cross
validation.}
\item{validation.error.type}{Type of error that validation was performed on.
One of \code{c("WMAE","WMSE","MAE","MSE")}.}
\item{gammas}{Vector of hyperparameter values tested during validation. This
vector will always be returned in descending order, regardless of the
ordering provided by the user. The indices \code{i.min} and \code{i.1se} correspond to
this descending ordering.}
\item{errors}{Vector of cross validation errors for the given hyperparameter
values.}
\item{se.errors}{The standard errors of the cross validation errors.
These are particularly useful for implementing the ``1-standard-error rule''.
The 1-SE rule favors a smoother trend filtering estimate by, instead of
using the hyperparameter that minimizes the CV error, instead uses the
largest hyperparameter that has a CV error within 1 standard error of the
smallest CV error.}
\item{gamma.min}{Hyperparameter value that minimizes the SURE error curve.}
\item{gamma.1se}{The largest hyperparameter value that is still within one
standard error of the minimum hyperparameter's cross validation error.}
\item{gamma.choice}{One of \code{c("gamma.min","gamma.1se")}. The choice
of hyperparameter that is used for optimized trend filtering estimate.}
\item{edfs}{Vector of effective degrees of freedom for trend filtering
estimators fit during validation.}
\item{edf.min}{The effective degrees of freedom of the optimally-tuned trend
filtering estimator.}
\item{edf.1se}{The effective degrees of freedom of the 1-stand-error rule
trend filtering estimator.}
\item{i.min}{The index of `gammas` that minimizes the cross validation error.}
\item{i.1se}{The index of `gammas` that gives the largest hyperparameter
value that has a cross validation error within 1 standard error of the
minimum of the cross validation error curves.}
\item{x}{Vector of observed inputs.}
\item{y}{Vector of observed outputs.}
\item{weights}{A vector of weights for the observed outputs. These are
defined as `weights = 1 / sigmas^2`, where `sigmas` is a vector of
standard errors of the uncertainty in the observed outputs.}
\item{fitted.values}{The optimized trend filtering estimate of the signal,
evaluated at the observed inputs `x`.}
\item{residuals}{`residuals = y - fitted.values`}
\item{k}{The degree of the trend filtering estimator.}
\item{optimization.params}{A list of parameters that control the trend
filtering convex optimization.}
\item{n.iter}{Vector of the number of iterations needed for the ADMM
algorithm to converge within the given tolerance, for each hyperparameter
value. If many of these are exactly equal to `max_iter`, then their
solutions have not converged with the tolerance specified by `obj_tol`.
In which case, it is often prudent to increase `max_iter`.}
\item{thinning}{Logical. If `TRUE`, then the data are preprocessed so
that a smaller, better conditioned data set is used for fitting.}
\item{x.scale, y.scale, data.scaled}{For internal use.}
}
\description{
\code{cv.trendfilter} optimizes the trend filtering hyperparameter
by performing V-fold cross validation on a vector, \code{gammas}, of candidate
hyperparameter values. The full generalization error curve and the optimized
trend filtering estimate are then returned as elements of a list object that
provides a comprehensive summary of the analysis.
}
\details{
\loadmathjax As a general rule-of-thumb, we recommend optimizing the
trend filtering hyperparameter by minimizing Stein's unbiased risk estimate
(using \code{\link{SURE.trendfilter}}) when the inputs are regularly-sampled
(either on the raw \code{x} scale or some transformation of it) and optimizing the
hyperparameter by \mjseqn{V}-fold cross validation when
the inputs are irregularly-sampled. A regularly-sampled data set with some
discarded pixels (either sporadically or in large consecutive chunks) is
still considered regularly sampled. When the inputs are regularly
sampled on a transformed scale, we recommend transforming to that
scale and carrying out the full trend filtering analysis (using SURE) on that
scale. When the inputs do not meet any of these criteria for ``regularly
sampled'', \code{cv.trendfilter} should be used to optimize the trend filtering
hyperparameter. Below we define the various types of validation error that
can be used with `cv.trendfilter` by passing the appropriate string
(one of `c("WMAE","WMSE","MAE","MSE")`) to the `validation.error.type`
argument. For the weighted validation errors, `weights` must be passed.
\mjsdeqn{WMAE(\gamma) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \gamma)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
\mjsdeqn{WMSE(\gamma) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \gamma)|^2\frac{w_i}{\sum_jw_j}}
\mjsdeqn{MAE(\gamma) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \gamma)|}
\mjsdeqn{MSE(\gamma) = \frac{1}{n}\sum_{i=1}^{n} |Y_i - \widehat{f}(x_i; \gamma)|^2}
where \mjseqn{w_i} is the \mjseqn{i}th element of the `weights` vector.
}
\examples{
data(eclipsing_binary)

opt <- cv.trendfilter(EB$phase, EB$flux, 1 / EB$std.err ^ 2,
                      validation.error.type = "MAE",
                      optimization.params = list(max_iter = 5e3, obj_tol = 1e-6, thinning = T))
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Cross validation}
\enumerate{
\item{Hastie, Tibshirani, and Friedman (2009).
\href{https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf}{
The Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd edition. Springer Series in Statistics. (See Sections 7.10
and 7.12)} \cr
\item{James, Witten, Hastie, and Tibshirani (2013).
\href{https://www.statlearning.com/}{An Introduction to Statistical Learning:
with Applications in R}. Springer. (See Section 5.1; Less technical than
ESL)} \cr
\item{Tibshirani (2013).
\href{https://www.stat.cmu.edu/~ryantibs/datamining/lectures/19-val2.pdf}{
Model selection and validation 2: Model assessment, more cross-validation}.
\emph{36-462: Data Mining course notes} (Carnegie Mellon University).}}
}
\seealso{
\code{\link{SURE.trendfilter}}, \code{\link{bootstrap.trendfilter}}
}
\author{
\subsection{\bold{Collin A. Politsch, Ph.D.}}{

Email: collinpolitsch@gmail.com \cr
Website: \href{https://collinpolitsch.com/}{collinpolitsch.com} \cr
GitHub: \href{https://github.com/capolitsch/}{github.com/capolitsch} \cr \cr
}
}
