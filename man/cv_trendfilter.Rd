% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hyperparameter-tuning.R
\name{cv_trendfilter}
\alias{cv_trendfilter}
\title{Optimize a trend filtering model by V-fold cross validation}
\usage{
cv_trendfilter(
  x,
  y,
  weights = NULL,
  nlambda = 250L,
  V = 10L,
  mc_cores = V,
  loss_funcs = NULL,
  fold_ids = NULL,
  ...
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal variance of the
additive noise that contaminates the output signal. When the noise is
expected to have an equal variance \mjseqn{\sigma^2} for all observations,
a scalar may be passed to \code{weights}, i.e. \verb{weights = }\mjseqn{1/\sigma^2}.
Otherwise, \code{weights} must be a vector with the same length as \code{x} and \code{y}.}

\item{nlambda}{Number of hyperparameter values to evaluate during cross validation.
Defaults to \code{nlambda = 250}. The hyperparameter grid is internally
constructed to span the full trend filtering model space (which is
bookended by a global polynomial solution and an interpolating solution),
with \code{nlambda} controlling the granularity of the hyperparameter grid.}

\item{V}{Number of folds that the data are partitioned into for V-fold cross
validation. Must be at least 2 and no more than 10. Defaults to \code{V = 10}.}

\item{mc_cores}{Number of cores to utilize for parallel computing. Defaults to
\code{mc_cores = V}.}

\item{loss_funcs}{A named list of one or more functions, with each defining a loss function
to be evaluated during cross validation. See the \strong{Loss functions} section
below for an example.}

\item{fold_ids}{An integer vector defining a custom partition of the data for cross
validation. \code{fold_ids} must have the same length as \code{x} and \code{y}, and only
contain integer values \code{1}, ..., \code{V} designating the fold assignments.}

\item{...}{Additional named arguments to pass to \code{\link[=.trendfilter]{.trendfilter()}}.}
}
\value{
An object of class '\code{cv_trendfilter}'. The object has subclass
'\code{\link[=trendfilter]{trendfilter}}' and may therefore be passed to generic
stats functions such as \code{\link[=predict]{predict()}}, \code{\link[=fitted]{fitted()}}, and \code{\link[=residuals]{residuals()}}.
And more precisely, a \code{cv_trendfilter}' object is a list with the elements
below, as well as all elements from a \code{\link[=trendfilter]{trendfilter}}' call
on the full data set.
\describe{
\item{\code{lambda}}{Vector of candidate hyperparameter values (always returned
in descending order).}
\item{\code{edf}}{Number of effective degrees of freedom in the trend filtering
estimator, for every candidate hyperparameter value in \code{lambda}.}
\item{\code{error}}{A named list of vectors, with each representing the
CV error curve for every loss function in \code{loss_funcs} (see below).}
\item{\code{se_error}}{Standard errors for the \code{error}, within a named list of
the same structure.}
\item{\code{lambda_min}}{A named vector with length equal to \code{length(lambda)},
containing the hyperparameter value that minimizes the CV error curve, for
every loss function in \code{loss_funcs}.}
\item{\code{lambda_1se}}{A named vector with length equal to \code{length(lambda)},
containing the "1-standard-error rule" hyperparameter, for every loss
function in \code{loss_funcs}. The "1-standard-error rule" hyperparameter is the
largest hyperparameter value in \code{lambda} that has a CV error within one
standard error of \code{min(error)}. It serves as an Occam's razor-like
heuristic. More precisely, given two models with approximately equal
performance (in terms of some loss function), it may be wise to opt for the
simpler model, i.e. the model with the larger hyperparameter value / fewer
effective degrees of freedom.}
\item{\code{edf_min}}{A named vector with length equal to \code{length(lambda)},
containing the number of effective degrees of freedom in the trend filtering
estimator that minimizes the CV error curve, for every loss function in
\code{loss_funcs}.}
\item{\code{edf_1se}}{A named vector with length equal to \code{length(lambda)},
containing the number of effective degrees of freedom in the
"1-standard-error rule" trend filtering estimator, for every loss function in
\code{loss_funcs}.}
\item{\code{i_min}}{A named vector with length equal to \code{length(lambda)},
containing the index of \code{lambda} that minimizes the CV error curve, for
every loss function in \code{loss_funcs}.}
\item{\code{i_1se}}{A named vector with length equal to \code{length(lambda)},
containing the index of \code{lambda} that gives the "1-standard-error rule"
hyperparameter value, for every loss function in \code{loss_funcs}.}
\item{\code{fitted_values}}{The fitted values of all trend filtering estimates,
return as a matrix with \code{length(lambda)} columns, with \code{fitted_values[,i]}
corresponding to the trend filtering estimate with hyperparameter
\code{lambda[i]}.
}
\item{\code{admm_params}}{A list of the parameter values used by the ADMM
algorithm used to solve the trend filtering convex optimization.}
\item{\code{obj_func}}{The relative change in the objective function over the
ADMM algorithm's final iteration, for every candidate hyperparameter in
\code{lambda}.}
\item{\code{n_iter}}{Total number of iterations taken by the ADMM algorithm, for
every candidate hyperparameter in \code{lambda}. If an element of \code{n_iter} is
exactly equal to \code{admm_params$max_iter}, then the ADMM algorithm stopped
before reaching the objective tolerance \code{admm_params$obj_tol}. In these
situations, you may need to increase the maximum number of tolerable
iterations by passing a \code{max_iter} argument to \code{cv_trendfilter()} in order
to ensure that the ADMM solution has converged to satisfactory precision.}
\item{\code{loss_funcs}}{A named list of functions that defines all loss functions
--- both internal and user-passed --- evaluated during cross validation.}
\item{\code{V}}{The number of folds the data were split into for cross
validation.}
\item{\code{x}}{Vector of observed values for the input variable.}
\item{\code{y}}{Vector of observed values for the output variable (if originally
present, observations with \code{is.na(y)} or \code{weights == 0} are dropped).}
\item{\code{weights}}{Vector of weights for the observed outputs.}
\item{\code{k}}{Degree of the trend filtering estimates.}
\item{\code{status}}{For internal use. Output from the C solver.}
\item{\code{call}}{The function call.}
\item{\code{scale_xy}}{For internal use.}
}
}
\description{
\loadmathjax For every candidate hyperparameter value, estimate the trend
filtering model's out-of-sample error by \mjseqn{V}-fold cross validation.
See the \strong{Details} section for guidelines on when \code{\link[=cv_trendfilter]{cv_trendfilter()}}
should be used versus \code{\link[=sure_trendfilter]{sure_trendfilter()}}.
}
\details{
Many common regression loss functions are defined internally, and a
cross validation curve is returned for each. Custom loss functions may also
be passed via the \code{loss_funcs} argument. See the \strong{Loss functions} section
below for definitions of the internal loss functions.
Generic functions such as \code{\link[=predict]{predict()}}, \code{\link{fitted}}, and \code{\link[=residuals]{residuals()}} may
be called on the \code{\link[=cv_trendfilter]{cv_trendfilter()}} output.

Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} versus
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are summarized in the table below. See Section 3.5 of
\href{https://arxiv.org/abs/1908.07151}{Politsch et al. (2020a)} for more details.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and measurement variances for \code{y} are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and measurement variances for \code{y} are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in wide consecutive chunks) is still considered to
be evenly sampled. When \code{x} is evenly sampled on a transformed scale, we
recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the \code{\link[=sure_trendfilter]{sure_trendfilter()}} examples for
a case when the inputs are evenly sampled on the \code{log10(x)} scale.
}
\section{Loss functions}{


The following loss functions are automatically computed during cross
validation and their CV error curves are returned in the \code{error} list
within the \code{\link[=cv_trendfilter]{cv_trendfilter()}} output.
\enumerate{
\item Mean absolute deviations error: \mjsdeqn{MAE(\lambda) =
\frac{1}{n} \sum_{i=1}^{n}|Y_i - \hat{f}(x_i; \lambda)|}
\item Weighted mean absolute deviations error:
\mjsdeqn{WMAE(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
\item Mean-squared error: \mjsdeqn{MSE(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|^2}
\item Weighted mean-squared error: \mjsdeqn{WMSE(\lambda)
= \sum_{i=1}^{n}|Y_i - \hat{f}(x_i; \lambda)|^2\frac{w_i}{\sum_jw_j}}
\item log-cosh error: \mjsdeqn{logcosh(\lambda) =
\frac{1}{n}\sum_{i=1}^{n}
\log\left(\cosh\left(Y_i - \hat{f}(x_i; \lambda)\right)\right)}
\item Weighted log-cosh error: \mjsdeqn{wlogcosh(\lambda) =
\sum_{i=1}^{n}
\log\left(\cosh\left((Y_i - \hat{f}(x_i; \lambda))\sqrt{w_i}\right)\right)}
\item Huber loss: \mjsdeqn{Huber(\lambda) =
\frac{1}{n}\sum_{i=1}^{n}L_{\lambda}(Y_i; \delta)}
\mjsdeqn{\text{where}\;\;\;\;L_{\lambda}(Y_i; \delta) = \cases{
|Y_i - \hat{f}(x_i; \lambda)|^2, &
$|Y_i - \hat{f}(x_i; \lambda)| \leq \delta$ \cr
2\delta|Y_i - \hat{f}(x_i; \lambda)| - \delta^2, &
$|Y_i - \hat{f}(x_i; \lambda)| > \delta$}}
\item Weighted Huber loss: \mjsdeqn{wHuber(\lambda) =
\sum_{i=1}^{n}L_{\lambda}(Y_i; \delta)}
\mjsdeqn{\text{where}\;\;\;\;L_{\lambda}(Y_i; \delta) = \cases{
|Y_i - \hat{f}(x_i; \lambda)|^2w_i, &
$|Y_i - \hat{f}(x_i; \lambda)|\sqrt{w_i} \leq \delta$ \cr
2\delta|Y_i - \hat{f}(x_i; \lambda)|\sqrt{w_i} -
\delta^2, & $|Y_i - \hat{f}(x_i; \lambda)|\sqrt{w_i} > \delta$}}
\item Mean-squared logarithmic error: \mjsdeqn{MSLE(\lambda) =
\frac{1}{n}\sum_{i=1}^{n}
\left|\log(Y_i + 1) - \log(\hat{f}(x_i; \lambda) + 1)\right|}
}

where \mjseqn{w_i:=} \code{weights[i]}.

When defining custom loss functions, each function within the named list
passed to \code{loss_funcs} should take three vector arguments --- \code{y},
\code{tf_estimate}, and \code{weights} --- and return a single scalar value for the
validation loss. For example, if we wanted to optimize the hyperparameter by
minimizing an uncertainty-weighted median of the model's MAD errors, we would
pass the list below to \code{loss_funcs}:\if{html}{\out{<div class="sourceCode r">}}\preformatted{MedAE <- function(tf_estimate, y, weights) \{
  matrixStats::weightedMedian(abs(tf_estimate - y), sqrt(weights))
\}

my_loss_funcs <- list(MedAE = MedAE)
}\if{html}{\out{</div>}}
}

\examples{
data("eclipsing_binary")
head(eclipsing_binary)

x <- eclipsing_binary$phase
y <- eclipsing_binary$flux
weights <- 1 / eclipsing_binary$std_err^2

cv_tf <- cv_trendfilter(
  x = x,
  y = y,
  weights = weights,
  max_iter = 1e4,
  obj_tol = 1e-6
)
}
\references{
\enumerate{
\item Politsch et al. (2020a). Trend filtering – I. A modern statistical tool
for time-domain astronomy and astronomical spectroscopy. \emph{MNRAS}, 492(3),
p. 4005-4018.
[\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{Publisher}]
[\href{https://arxiv.org/abs/1908.07151}{arXiv}].
\item Politsch et al. (2020b). Trend Filtering – II. Denoising astronomical
signals with varying degrees of smoothness. \emph{MNRAS}, 492(3), p. 4019-4032.
[\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{Publisher}]
[\href{https://arxiv.org/abs/2001.03552}{arXiv}].
}
}
