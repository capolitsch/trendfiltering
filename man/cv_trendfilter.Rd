% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_trendfilter.R
\name{cv_trendfilter}
\alias{cv_trendfilter}
\title{Optimize the trend filtering hyperparameter by \emph{V}-fold cross validation}
\usage{
cv_trendfilter(
  x,
  y,
  weights,
  k = 2L,
  nlambdas = 250L,
  V = 10L,
  mc_cores = V,
  loss_funcs,
  fold_ids,
  optimization_params
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{(Optional) Weights for the observed outputs, defined as the
reciprocal variance of the additive noise that contaminates the output
signal. When the noise is expected to have an equal variance,
\mjseqn{\sigma^2}, for all observations, a scalar may be passed to \code{weights},
namely \verb{weights = }\mjseqn{1/\sigma^2}. Otherwise, \code{weights} must be a vector
with the same length as \code{x} and \code{y}.}

\item{k}{Degree of the polynomials that make up the piecewise-polynomial
trend filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{Number of hyperparameter values to test during validation.
Defaults to \code{nlambdas = 250}. The hyperparameter grid is internally
constructed to span the full trend filtering model space (which is bookended
by a global polynomial solution and an interpolating solution), with
\code{nlambdas} controlling the granularity of the hyperparameter grid.}

\item{V}{Number of folds that the data are partitioned into for \emph{V}-fold
cross validation. Defaults to \code{V = 10}.}

\item{mc_cores}{Multi-core computing using the
\code{\link[parallel:parallel-package]{parallel}} R package: The number of cores to
utilize. Defaults to the number of cross validation folds, \code{mc_cores = V}.
If the value passed to \code{mc_cores} exceeds the number of cores available on
the machine, \code{mc_cores} is internally updated to
\code{mc_cores = parallel::detectCores()}.}

\item{loss_funcs}{(Optional) A named list of one or more functions, with each
defining a loss function that will give rise to its own CV error curve. Mean
absolute deviations error (MAE), mean-squared error (MSE), Huber loss,
mean-squared logarithmic error (MSLE), as well as observation-weighted
versions of each, are all computed internally and returned. Therefore, the
\code{loss_funcs} argument need only be used to define loss functions that are not
among these common choices.

Each function within the named list passed to \code{loss_funcs} should take three
vector arguments --- \code{y}, \code{tf_estimate}, and \code{weights} --- and return a
single scalar value for the validation error. For example, if I wanted a CV
error curve based on a weighted median of the absolute errors to be computed,
I would pass the list below to \code{loss_funcs}:\if{html}{\out{<div class="sourceCode r">}}\preformatted{MedAE <- function(tf_estimate, y, weights) \{
  matrixStats::weightedMedian(abs(tf_estimate - y), sqrt(weights))
\}

list(MedAE = MedAE)
}\if{html}{\out{</div>}}}

\item{fold_ids}{(Optional) An integer vector defining a custom partition of
the data for cross validation. \code{fold_ids} must have the same length as \code{x}
and \code{y}, and only contain values in the set \code{1:V}.}

\item{optimization_params}{(Optional) A named list of parameter values to be
passed to the trend filtering ADMM algorithm of
\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and Tibshirani (2016)}, which is implemented in
the \code{glmgen} R package. See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for a full list of the algorithm's parameters. The default
parameter choices will almost always suffice, but when adjustments do need to
be made, one can do so without any technical understanding of the ADMM
algorithm. In particular, the four parameter descriptions below should serve
as sufficient working knowledge.
\describe{
\item{obj_tol}{A stopping threshold for the ADMM algorithm. If the relative
change in the algorithm's cost functional between two consecutive steps is
less than \code{obj_tol}, the algorithm terminates. The algorithm's termination
can also result from it reaching the maximum tolerable iterations set
by the \code{max_iter} parameter (see below). The \code{obj_tol} parameter defaults to
\code{obj_tol = 1e-10}. The \code{cost_functional} vector, returned within the
\code{sure_trendfilter()} output, gives the relative change in the trend filtering
cost functional over the algorithm's final iteration, for every candidate
hyperparameter value.}
\item{max_iter}{Maximum number of ADMM iterations that we will tolerate.
Defaults to \code{max_iter = length(y)}. The actual number of iterations performed
by the algorithm, for every candidate hyperparameter value, is returned in
the \code{n_iter} vector, within the \code{sure_trendfilter()} output. If any of the
elements of \code{n_iter} are equal to \code{max_iter}, the tolerance defined by
\code{obj_tol} has not been attained and \code{max_iter} may need to be increased.}
\item{thinning}{Logical. If \code{thinning = TRUE}, then the data are preprocessed
so that a smaller data set is used to fit the trend filtering estimate, which
will ease the ADMM algorithm's convergence. This can be
very useful when a signal is so well-sampled that very little additional
information / predictive accuracy is gained by fitting the trend filtering
estimate on the full data set, compared to some subset of it. See the
\code{\link[=cv_trendfilter]{cv_trendfilter()}} examples for a case study of this nature. When nothing
is passed to \code{thinning}, the algorithm will automatically detect whether
thinning should be applied. This preprocessing procedure is controlled by the
\code{x_tol} parameter below.}
\item{x_tol}{Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then the data is thinned.
}}}
}
\value{
An object of class \code{'cv_tf'}. This is a list with the following
elements:
\describe{
\item{lambdas}{Vector of candidate hyperparameter values (always returned in
descending order).}
\item{edfs}{Number of effective degrees of freedom in the trend filtering
estimator, for every candidate hyperparameter value in \code{lambdas}.}
\item{cv_errors}{A named list of vectors, with each representing the
cross validation error curve for a given loss function. The first 8
vectors of the list correspond to MAE, WMAE, MSE, WMSE, Huber loss, weighted
Huber loss, MSLE, and WMSLE. If any custom loss functions were passed to
\code{loss_funcs}, their cross validation curves will follow the first 8.}
\item{se_cv_errors}{Standard errors for each of the cross validation error
curves in \code{cv_errors}, within a named list of the same structure.}
\item{lambda_min}{A named vector with length equal to \code{length(cv_errors)},
containing the hyperparameter value that minimizes the cross validation error
curve, for every loss function.}
\item{lambda_1se}{A named vector with length equal to \code{length(cv_errors)},
containing the "1-standard-error rule" hyperparameter, for every loss
function. The "1-standard-error rule" hyparameter is the largest
hyperparameter value (corresponding to the smoothest trend filtering
estimate) that has a CV error within one standard error of the minimum CV
error. It serves as an Occam's razor-like heuristic. That is, given two
models with approximately equal performance, it may be wise to opt for the
simpler model, i.e. the model with fewer effective degrees of freedom.}
\item{edf_min}{A named vector with length equal to \code{length(cv_errors)},
containing the number of effective degrees of freedom in the trend filtering
estimator that minimizes the CV error curve, for every loss function.}
\item{edf_1se}{A named vector with length equal to \code{length(cv_errors)},
containing the number of effective degrees of freedom in the
"1-standard-error rule" trend filtering estimator, for every type of
validation error.}
\item{i_min}{A named vector with length equal to \code{length(cv_errors)},
containing the index of \code{lambdas} that yields the minimum of the CV error
curve, for every loss function.}
\item{i_1se}{A named vector with length equal to \code{length(cv_errors)},
containing the index of \code{lambdas} that gives the "1-standard-error rule"
hyperparameter value, for every loss function.}
\item{cv_loss_funcs}{A named list of functions that defines all loss
functions evaluated during cross validation.}
\item{cost_functional}{The relative change in the cost functional over the
ADMM algorithm's final iteration, for every candidate hyperparameter in
\code{lambdas}.}
\item{n_iter}{Total number of iterations taken by the ADMM algorithm, for
every candidate hyperparameter in \code{lambdas}. If an element of \code{n_iter}
is exactly equal to the value set by \code{optimization_params$max_iter}, then the
ADMM algorithm stopped before reaching the tolerance set by \code{obj_tol}. In
these situations, you may need to increase \code{max_iter} to ensure the trend
filtering solution has converged with satisfactory precision.}
\item{V}{The number of folds the data were split into for cross validation.}
\item{tf_model}{A list of objects that is used internally by other functions
that operate on the \code{cv_trendfilter()} output.}
}
}
\description{
\loadmathjax For every candidate hyperparameter value, estimate the trend
filtering model's out-of-sample error by \emph{V}-fold cross validation. CV
error curves are returned for four of the most common regression loss
metrics, as well as observation-weighted versions. Custom loss functions may
also be passed to the \code{loss_funcs} argument. See the details section for
definitions of the internal loss functions, and for guidelines on when
\code{\link[=cv_trendfilter]{cv_trendfilter()}} should be used versus \code{\link[=sure_trendfilter]{sure_trendfilter()}}.
}
\details{
Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} versus
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are shown in the table below.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in large consecutive chunks) is still considered to
be evenly sampled. When the inputs are evenly sampled on a transformed scale,
we recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the \code{\link[=sure_trendfilter]{sure_trendfilter()}} examples for
a case when the inputs are evenly sampled on the \code{log10(x)} scale.

The following loss functions are automatically computed during cross
validation and their CV error curves are returned within the \code{cv_errors}
list of the \code{'cv_tf'} output object.

\enumerate{
\item Mean-squared error: \mjsdeqn{\text{MSE}(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|^2}
\item Weighted mean-squared error: \mjsdeqn{\text{WMSE}(\lambda)
= \sum_{i=1}^{n}|Y_i - \hat{f}(x_i; \lambda)|^2\frac{w_i}{\sum_jw_j}}
\item Mean absolute deviations error: \mjsdeqn{\text{MAE}(\lambda) =
\frac{1}{n} \sum_{i=1}^{n}|Y_i - \hat{f}(x_i; \lambda)|}
\item Weighted mean absolute deviations error:
\mjsdeqn{\text{WMAE}(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
\item Huber error: \mjsdeqn{\text{Huber}(\lambda) =
\frac{1}{n}\sum_{i=1}^{n}L_{\delta}(Y_i; \lambda)}
\mjsdeqn{\text{where}\;\;\;\;L_{\delta}(Y_i; \lambda) = \cases{
|Y_i - \hat{f}(x_i; \lambda)|^2, &
$|Y_i - \hat{f}(x_i; \lambda)| \leq \delta$ \cr
2\delta|Y_i - \hat{f}(x_i; \lambda)| - \delta^2, &
$|Y_i - \hat{f}(x_i; \lambda)| > \delta$}}
\item Weighted Huber error: \mjsdeqn{\text{wHuber}(\lambda) =
\sum_{i=1}^{n}L_{\delta}(Y_i; \lambda)}
\mjsdeqn{\text{where}\;\;\;\;L_{\delta}(Y_i; \lambda) = \cases{
|Y_i - \hat{f}(x_i; \lambda)|^2, &
$|Y_i - \hat{f}(x_i; \lambda)|\sqrt{w_i} \leq \delta$ \cr
2\delta|Y_i - \hat{f}(x_i; \lambda)| -
\delta^2, & $|Y_i - \hat{f}(x_i; \lambda)|\sqrt{w_i} > \delta$}}
\item Mean-squared logarithmic error: \mjsdeqn{\text{MSLE}(\lambda) =
\frac{1}{n}\sum_{i=1}^{n}
\left|\log(Y_i + 1) - \log(\hat{f}(x_i; \lambda) + 1)\right|}
\item Weighted mean-squared logarithmic error: \mjsdeqn{\text{MSLE}(\lambda)
= \frac{1}{n}\sum_{i=1}^{n}
\left|\log(Y_i + 1) - \log(\hat{f}(x_i; \lambda) + 1)\right|\log(w_i + 1)}
}
where \mjseqn{w_i:=} \code{weights[i]}.

If constant weights are passed, or if nothing is passed, then a weighted loss
function is equivalent to its unweighted counterpart.
}
\examples{
data(eclipsing_binary)
head(EB)

cv_tf <- cv_trendfilter(
  x = EB$phase,
  y = EB$flux,
  weights = 1 / EB$std_err^2,
  optimization_params = list(
    max_iter = 1e4,
    obj_tol = 1e-6,
    thinning = TRUE
  )
)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Cross validation}
\enumerate{
\item{Hastie, Tibshirani, and Friedman (2009).
\href{https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf}{
The Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd edition. Springer Series in Statistics. (See Sections 7.10
and 7.12)}}
}
\seealso{
\code{\link[=sure_trendfilter]{sure_trendfilter()}}, \code{\link[=bootstrap_trendfilter]{bootstrap_trendfilter()}}
}
