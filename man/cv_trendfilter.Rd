% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_trendfilter.R
\name{cv_trendfilter}
\alias{cv_trendfilter}
\title{Optimize the trend filtering hyperparameter by V-fold cross validation}
\usage{
cv_trendfilter(
  x,
  y,
  weights,
  k = 2L,
  nlambdas = 250L,
  V = 10L,
  mc_cores = parallel::detectCores() - 4,
  custom_error_funcs,
  optimization_params
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal in \code{y}.
When the noise is expected to have equal variance for all observations,
\code{weights} can be passed as a scalar. Otherwise, \code{weights} must be a vector
with the same length as \code{x} and \code{y}.}

\item{k}{Degree of the polynomials that make up the piecewise-polynomial
trend filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{Number of hyperparameter values to test during validation.
Defaults to \code{nlambdas = 250}. The hyperparameter grid is dynamically
constructed to span the full model space lying between a single polynomial
solution (i.e. a power law) and an interpolating solution, with \code{nlambdas}
controlling the granularity of the hyperparameter grid.}

\item{V}{Number of folds that the data are partitioned into for the V-fold
cross validation. Defaults to \code{V = 10}.}

\item{mc_cores}{Multi-core computing using the
\code{\link[parallel:parallel-package]{parallel}} R package: The number of cores to
utilize. Defaults to the number of cores detected, minus four.}

\item{custom_error_funcs}{(Optional) A named list of one or more
functions, with each defining an error functional to evaluate on held-out
folds during cross validation. Mean-squared error (MSE) and mean absolute
error (MAE) are both evaluated automatically, as well as weighted versions
(with reciprocal variances as weights) --- WMSE and WMAE. Therefore, the user
does not need to pass anything to \code{custom_error_funcs} unless they want to
define a validation error metric other than MSE, MAE, WMSE, and WMAE.

In such a case, each function in the named list passed to
\code{custom_error_funcs} should take three vector arguments --- \code{y},
\code{tf_estimate}, and \code{weights} --- and return a single scalar value for the
validation error. For example, if I also wanted to define a validation
error curve from the weighted median of the absolute errors, I could pass the
following list to \code{custom_error_funcs}:\if{html}{\out{<div class="sourceCode r">}}\preformatted{list(MedAE = function(tf_estimate, y, weights) \{
               matrixStats::weightedMedian(abs(tf_estimate - y), weights)
             \}
    )
}\if{html}{\out{</div>}}}

\item{optimization_params}{(Optional) A named list of parameter values to be
passed to the trend filtering ADMM algorithm of
\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and Tibshirani (2016)}, which is implemented in
the \code{glmgen} R package. See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for a full list of the algorithm's parameters. The default
parameter choices will almost always suffice, but when adjustments do need to
be made, one can do so without any technical understanding of the ADMM
algorithm. In particular, the four parameter descriptions below should serve
as sufficient working knowledge.
\describe{
\item{obj_tol}{A stopping threshold for the ADMM algorithm. If the relative
change in the algorithm's cost functional between two consecutive steps is
less than \code{obj_tol}, the algorithm terminates. The algorithm's termination
can also result from it reaching the maximum tolerable iterations set
by the \code{max_iter} parameter (see below). The \code{obj_tol} parameter defaults to
\code{obj_tol = 1e-10}. The \code{cost_functional} vector, returned within the
\code{sure_trendfilter()} output, gives the relative change in the trend filtering
cost functional over the algorithm's final iteration, for every candidate
hyperparameter value.}
\item{max_iter}{Maximum number of ADMM iterations that we will tolerate.
Defaults to \code{max_iter = length(y)}. The actual number of iterations performed
by the algorithm, for every candidate hyperparameter value, is returned in
the \code{n_iter} vector, within the \code{sure_trendfilter()} output. If any of the
elements of \code{n_iter} are equal to \code{max_iter}, the tolerance defined by
\code{obj_tol} has not been attained and \code{max_iter} may need to be increased.}
\item{thinning}{Logical. If \code{thinning = TRUE}, then the data are preprocessed
so that a smaller data set is used to fit the trend filtering estimate, which
will ease the ADMM algorithm's convergence. This can be
very useful when a signal is so well-sampled that very little additional
information / predictive accuracy is gained by fitting the trend filtering
estimate on the full data set, compared to some subset of it. See the
\code{\link[=cv_trendfilter]{cv_trendfilter()}} examples for a case study of this nature. When nothing
is passed to \code{thinning}, the algorithm will automatically detect whether
thinning should be applied. This preprocessing procedure is controlled by the
\code{x_tol} parameter below.}
\item{x_tol}{Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then the data is thinned.
}}}
}
\value{
An object of class \code{cv_tf}. This is a list with the following
elements:
\describe{
\item{lambdas}{Vector of candidate hyperparameter values (always returned in
descending order).}
\item{edfs}{Number of effective degrees of freedom in the trend filtering
estimator, for every hyperparameter value in \code{lambdas}.}
\item{validation_errors}{A named list of vectors, with each representing the
cross validation error curve for some definition of error. The first 4
vectors of the list correspond to WMAE, WMSE, MAE, MSE. If any custom
error functionals were passed to \code{custom_error_funcs}, their cross validation
curves will follow the first 4.}
\item{se_validation_errors}{Named list of estimated standard errors for each
of the cross validation error curves in \code{validation_errors}.}
\item{lambda_min}{A named vector with length equal to
\code{length(validation_errors)}, containing the hyperparameter value that
minimizes the cross validation error curve, for every type validation error.}
\item{lambda_1se}{A named vector with length equal to
\code{length(validation_errors)}, containing the "1-standard-error rule"
hyperparameter, for every type validation error. The "1-standard-error rule"
hyparameter is the largest hyperparameter value (corresponding to the
smoothest trend filtering estimate) that has a cross validation error
within one standard error of the minimum cross validation error. It serves as
an Occam's razor-esque heuristic. That is, given two models with
approximately equal performance, it may be wise to opt for the simpler model,
i.e. the model with fewer effective degrees of freedom.}
\item{edf_min}{A named vector with length equal to
\code{length(validation_errors)}, containing the number of effective degrees of
freedom in the trend filtering estimator that minimizes the cross validation
error curve, for every type of validation error.}
\item{edf_1se}{A named vector with length equal to
\code{length(validation_errors)}, containing the number of effective degrees of
freedom in the "1-standard-error rule" trend filtering estimator, for every
type of validation error.}
\item{i_min}{A named vector with length equal to
\code{length(validation_errors)}, containing the index of \code{lambdas} that minimizes
the CV error curve, for every type of validation error.}
\item{i_1se}{A named vector with length equal to
\code{length(validation_errors)}, containing the index of \code{lambdas} that
gives the "1-standard-error rule" hyperparameter value, for every
type of validation error.}
\item{validation_error_funcs}{A named list of functions that define the
types of error that were evaluated during cross validation.}
\item{cost_functional}{The relative change in the cost functional over the
ADMM algorithm's final iteration, for every candidate hyperparameter in
\code{lambdas}.}
\item{n_iter}{Total number of iterations taken by the ADMM algorithm, for
every candidate hyperparameter in \code{lambdas}. If an element of \code{n_iter}
is exactly equal to \code{max_iter}, then the ADMM algorithm stopped before
reaching the tolerance set by \code{obj_tol}. In these cases, you may need
to increase \code{max_iter} to ensure the trend filtering solution has
converged to satisfactory precision.}
\item{V}{The number of folds the data were split into for cross validation.}
\item{model_fit}{A list of objects that is used internally by other
functions that operate on the \code{cv_trendfilter()} output.}
}
}
\description{
For every candidate hyperparameter value, estimate the corresponding trend
filtering model's out-of-sample error by V-fold cross validation. Four
commonly-used types of error --- MSE, WMSE, MAE, WMAE --- are all evaluated
and returned, and the user has the option to pass additional error
functionals to be evaluated. See the details section for when you should use
\code{\link[=cv_trendfilter]{cv_trendfilter()}} versus \code{\link[=sure_trendfilter]{sure_trendfilter()}}.
}
\details{
Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} versus
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are shown in the table below.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in large consecutive chunks) is still considered to
be evenly sampled. When the inputs are evenly sampled on a transformed scale,
we recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the \code{\link[=sure_trendfilter]{sure_trendfilter()}} examples for
a case when the inputs are evenly sampled on the \code{log10(x)} scale.

The validation error functionals that we automatically compute CV curves for
are formally defined below.

\loadmathjax
\mjsdeqn{MSE(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|^2}
\mjsdeqn{MAE(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|}
\mjsdeqn{WMSE(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|^2\frac{w_i}{\sum_jw_j}}
\mjsdeqn{WMAE(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
where \mjseqn{w_i:=} \code{weights[i]}.

If constant weights are passed, or if nothing is passed, then the weighted
and unweighted counterparts are equivalent.

Briefly stated, weighting the validation error metric (with reciprocal
variances) helps prevent the error from being dominated by the model's
(in)accuracy on a (potentially very small) subset of the data that have
high variance; and absolute error is less sensitive to outliers than squared
error.
}
\examples{
data(eclipsing_binary)
head(EB)

cv_tf <- cv_trendfilter(
  x = EB$phase,
  y = EB$flux,
  weights = 1 / EB$std_err^2,
  optimization_params = list(
    max_iter = 1e4,
    obj_tol = 1e-6,
    thinning = TRUE
  )
)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Cross validation}
\enumerate{
\item{Hastie, Tibshirani, and Friedman (2009).
\href{https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf}{
The Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd edition. Springer Series in Statistics. (See Sections 7.10
and 7.12)}}
}
\seealso{
\code{\link[=sure_trendfilter]{sure_trendfilter()}}, \code{\link[=bootstrap_trendfilter]{bootstrap_trendfilter()}}
}
