% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_trendfilter.R
\name{cv_trendfilter}
\alias{cv_trendfilter}
\title{Optimize the trend filtering hyperparameter by V-fold cross validation}
\usage{
cv_trendfilter(
  x,
  y,
  weights,
  k = 2L,
  nlambdas = 250L,
  V = 10L,
  validation_error,
  mc_cores = parallel::detectCores() - 4,
  optimization_params
)
}
\arguments{
\item{x}{Vector of observed values for the input variable.}

\item{y}{Vector of observed values for the output variable.}

\item{weights}{Weights for the observed outputs, defined as the reciprocal
variance of the additive noise that contaminates the signal in \code{y}.
When the noise is expected to have equal variance for all observations,
\code{weights} can be passed as a scalar. Otherwise, \code{weights} must be a vector
with the same length as \code{x} and \code{y}.}

\item{k}{Degree of the polynomials that make up the piecewise-polynomial
trend filtering estimate. Defaults to \code{k = 2} (i.e. a piecewise quadratic
estimate). Must be one of \verb{k = 0,1,2}. Higher order polynomials are
disallowed since their smoothness is indistinguishable from \code{k = 2} and
their use can lead to instability in the convex optimization.}

\item{nlambdas}{Number of hyperparameter values to test during validation.
Defaults to \code{nlambdas = 250}. The hyperparameter grid is dynamically
constructed to span the full model space lying between a single polynomial
solution (i.e. a power law) and an interpolating solution, with \code{nlambdas}
controlling the granularity of the grid.}

\item{V}{Number of folds that the data are partitioned into for the V-fold
cross validation. Defaults to \code{V = 10}.}

\item{validation_error}{The error functional to optimize during cross
validation. Some common choices can be used by passing an appropriate string
--- one of \code{c("MSE","MAE","WMSE","WMAE")}, i.e. mean-absolute deviations
error, mean-squared error, and their weighted counterparts. Defaults to
\code{validation_error = "WMAE"}.

Alternatively, custom validation error functionals can be used by instead
passing a function to \code{validation_error}. The function should take three
vector arguments --- \code{y}, \code{tf_estimate}, and \code{weights} --- and return a
single scalar value for the validation error. For example, the default
setting \code{validation_error = "WMAE"} is equivalent to passing the
following function to \code{validation_error}:\if{html}{\out{<div class="sourceCode r">}}\preformatted{function(tf_estimate, y, weights)\{
  sum(abs(tf_estimate - y) * sqrt(weights) / sum(sqrt(weights)))
\}
}\if{html}{\out{</div>}}}

\item{mc_cores}{Multi-core computing using the
\code{\link[parallel:parallel-package]{parallel}} R package: The number of cores to
utilize. Defaults to the number of cores detected, minus four.}

\item{optimization_params}{(Optional) A named list of parameter values to be
passed to the trend filtering ADMM algorithm of
\href{http://www.stat.cmu.edu/~ryantibs/papers/fasttf.pdf}{Ramdas and Tibshirani (2016)}, which is implemented in
the \code{glmgen} R package. See the \code{\link[glmgen:trendfilter.control.list]{glmgen::trendfilter.control.list()}}
documentation for a full list of the algorithm's parameters. The default
parameter choices will almost always suffice, but when adjustments do need to
be made, one can do so without any technical understanding of the ADMM
algorithm. In particular, the four parameter descriptions below should serve
as sufficient working knowledge.
\describe{
\item{obj_tol}{A stopping threshold for the ADMM algorithm. If the relative
change in the algorithm's cost functional between two consecutive steps is
less than \code{obj_tol}, the algorithm terminates. The algorithm's termination
can also result from it reaching the maximum tolerable iterations set
by the \code{max_iter} parameter (see below). The \code{obj_tol} parameter defaults to
\code{obj_tol = 1e-10}. The \code{cost_functional} vector, returned within the
\code{sure_trendfilter()} output, gives the relative change in the trend filtering
cost functional over the algorithm's final iteration, for every candidate
hyperparameter value.}
\item{max_iter}{Maximum number of ADMM iterations that we will tolerate.
Defaults to \code{max_iter = length(y)}. The actual number of iterations performed
by the algorithm, for every candidate hyperparameter value, is returned in
the \code{n_iter} vector, within the \code{sure_trendfilter()} output. If any of the
elements of \code{n_iter} are equal to \code{max_iter}, the tolerance defined by
\code{obj_tol} has not been attained and \code{max_iter} may need to be increased.}
\item{thinning}{Logical. If \code{thinning = TRUE}, then the data are preprocessed
so that a smaller data set is used to fit the trend filtering estimate, which
will ease the ADMM algorithm's convergence. This can be
very useful when a signal is so well-sampled that very little additional
information / predictive accuracy is gained by fitting the trend filtering
estimate on the full data set, compared to some subset of it. See the
\code{\link[=cv_trendfilter]{cv_trendfilter()}} examples for a case study of this nature. When nothing
is passed to \code{thinning}, the algorithm will automatically detect whether
thinning should be applied. This preprocessing procedure is controlled by the
\code{x_tol} parameter below.}
\item{x_tol}{Controls the automatic detection of when thinning should be
applied to the data. If we make bins of size \code{x_tol} and find at least two
elements of \code{x} that fall into the same bin, then the data is thinned.
}}}
}
\value{
An object of class \code{cv_tf}. This is a list with the following
elements:
\describe{
\item{lambdas}{Vector of candidate hyperparameter values (always returned in
descending order).}
\item{edfs}{Number of effective degrees of freedom in the trend filtering
estimator, for every hyperparameter value in \code{lambdas}.}
\item{validation_errors}{Cross validation errors, for every
hyperparameter value in \code{lambdas}.}
\item{se_validation_errors}{Vector of estimated standard errors for the
\code{validation_errors}.}
\item{lambda_min}{Hyperparameter value that minimizes the cross validation
error curve.}
\item{lambda_1se}{The largest hyperparameter value (corresponding to the
smoothest trend filtering estimate) that yields a cross validation error
within one standard error of \code{min(validation_errors)}. We call this the
"1-standard-error rule" hyperparameter, and it serves as an Occam's
razor-esque heuristic. That is, given two models with approximately equal
performance, it may be wise to opt for the simpler model, i.e. the model with
fewer effective degrees of freedom.}
\item{edf_min}{Number of effective degrees of freedom in the trend filtering
estimator that minimizes the cross validation error curve.}
\item{edf_1se}{Number of effective degrees of freedom in the 1-stand-error
rule trend filtering estimator.}
\item{i_min}{Index of \code{lambdas} that minimizes the CV error curve.}
\item{i_1se}{Index of \code{lambdas} that gives the 1-standard-error rule
hyperparameter.}
\item{cost_functional}{The relative change in the cost functional over the
ADMM algorithm's final iteration, for every candidate hyperparameter in
\code{lambdas}.}
\item{n_iter}{Total number of iterations taken by the ADMM algorithm, for
every candidate hyperparameter in \code{lambdas}. If an element of \code{n_iter}
is exactly equal to \code{max_iter}, then the ADMM algorithm stopped before
reaching the tolerance set by \code{obj_tol}. In these cases, you may need
to increase \code{max_iter} to ensure the trend filtering solution has
converged to satisfactory precision.}
\item{validation_error}{Type of error that validation was performed on.
Either one of \code{c("MSE","MAE","WMSE","WMAE")} or a custom function passed by
the user.}
\item{V}{The number of folds the data were split into for cross validation.}
\item{model_fit}{A list of objects that is used internally by other
functions that operate on the \code{cv_trendfilter()} output.}
}
}
\description{
For every candidate hyperparameter value, estimate the corresponding trend
filtering model's out-of-sample error by V-fold cross validation. Here, any
error metric can be defined. See details below for when you should use
\code{\link[=cv_trendfilter]{cv_trendfilter()}} versus \code{\link[=sure_trendfilter]{sure_trendfilter()}}.
}
\details{
Our recommendations for when to use \code{\link[=cv_trendfilter]{cv_trendfilter()}} versus
\code{\link[=sure_trendfilter]{sure_trendfilter()}} are shown in the table below.\tabular{lc}{
   Scenario \tab Hyperparameter optimization \cr
   \code{x} is unevenly sampled \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are not available \tab \code{\link[=cv_trendfilter]{cv_trendfilter()}} \cr
   \code{x} is evenly sampled and reciprocal variances are available \tab \code{\link[=sure_trendfilter]{sure_trendfilter()}} \cr
}


For our purposes, an evenly sampled data set with some discarded pixels
(either sporadically or in large consecutive chunks) is still considered to
be evenly sampled. When the inputs are evenly sampled on a transformed scale,
we recommend transforming to that scale and carrying out the full trend
filtering analysis on that scale. See the \code{\link[=sure_trendfilter]{sure_trendfilter()}} examples for
a case when the inputs are evenly sampled on the \code{log10(x)} scale.

The formal definitions of the common validation error functionals available
via the options \code{validation_error = c("MSE","MAE","WMSE","WMAE")} are
stated below.

\loadmathjax
\mjsdeqn{MSE(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|^2}
\mjsdeqn{MAE(\lambda) = \frac{1}{n}
\sum_{i=1}^{n} |Y_i - \hat{f}(x_i; \lambda)|}
\mjsdeqn{WMSE(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|^2\frac{w_i}{\sum_jw_j}}
\mjsdeqn{WMAE(\lambda) = \sum_{i=1}^{n}
|Y_i - \hat{f}(x_i; \lambda)|\frac{\sqrt{w_i}}{\sum_j\sqrt{w_j}}}
where \mjseqn{w_i:=} \code{weights[i]}.

If constant weights are passed, or if nothing is passed, then the weighted
and unweighted counterparts are equivalent.

Briefly stated, weighting the validation error metric (with reciprocal
variances) helps prevent the error from being dominated by the model's
(in)accuracy on a (potentially very small) subset of the data that have
high variance; and absolute error is less sensitive to outliers than squared
error.
}
\examples{
data(eclipsing_binary)
head(EB)

cv_tf <- cv_trendfilter(
  x = EB$phase,
  y = EB$flux,
  weights = 1 / EB$std_err^2,
  validation_error = "MAE",
  optimization_params = list(
    max_iter = 1e4,
    obj_tol = 1e-6,
    thinning = TRUE
  )
)
}
\references{
\bold{Companion references}
\enumerate{
\item{Politsch et al. (2020a).
\href{https://academic.oup.com/mnras/article/492/3/4005/5704413}{
Trend filtering – I. A modern statistical tool for time-domain astronomy and
astronomical spectroscopy}. \emph{MNRAS}, 492(3), p. 4005-4018.} \cr
\item{Politsch et al. (2020b).
\href{https://academic.oup.com/mnras/article/492/3/4019/5704414}{
Trend Filtering – II. Denoising astronomical signals with varying degrees of
smoothness}. \emph{MNRAS}, 492(3), p. 4019-4032.}}

\bold{Cross validation}
\enumerate{
\item{Hastie, Tibshirani, and Friedman (2009).
\href{https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf}{
The Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd edition. Springer Series in Statistics. (See Sections 7.10
and 7.12)}}
}
\seealso{
\code{\link[=sure_trendfilter]{sure_trendfilter()}}, \code{\link[=bootstrap_trendfilter]{bootstrap_trendfilter()}}
}
